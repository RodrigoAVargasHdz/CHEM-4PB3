{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regularization and Overfitting - Introduction**\n",
    "\n",
    "In the last [code](https://colab.research.google.com/github/RodrigoAVargasHdz/CHEM-4PB3/blob/w2024/Course_Notes/Week%203/Week_3_Linear_Regression.ipynb) lecture we saw that when we consider a high degree order polynomial regression model, there was a significant variation/oscillation in the prediction and it did not generalized when we predict beyong the trainig data regime. \n",
    "A possible reason is because the polynomial is too complex compared to the amount of data, $d >> N$. <br>\n",
    "In application, this can be avoided by:\n",
    "\n",
    "1. **Conditioning of the Problem**: High-degree polynomials can cause numerical instability and poor conditioning, leading to large oscillations in the fit.\n",
    "\n",
    "2. **Data Scaling**: Scaling both `x` and `y` data to a smaller range can improve the condition of the polynomial fitting and potentially reduce numerical errors.\n",
    "\n",
    "3. **Regularization**: Regularization techniques like reducing the polynomial degree can limit model complexity, but are less straightforward in polynomial fitting compared to linear models.\n",
    "\n",
    "\n",
    "> It is important to condition and regularize the data to avoid **overfitting** or **underfitting** the polynominal.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<a href=\"https://github.com/RodrigoAVargasHdz/CHEM-4PB3\" target=\"_blank\">\n",
    "  <img src=\"https://raw.githubusercontent.com/RodrigoAVargasHdz/CHEM-4PB3/Volume-2/Assests/overfitting_underfitting_graphs.png\" alt=\"Visualizing LogP\" width=\"700\">\n",
    "</a>\n",
    "<br>\n",
    "</a>\n",
    "\n",
    "**Figure 3 - Illustrating Overfitting versus Underfitting. Image by Rudra Sondhi.**\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    ">#### **Learn More:**\n",
    "1. **What is Overfitting in Deep Learning**: [Educational Resource](https://www.v7labs.com/blog/overfitting)\n",
    "2. **Regularization in Machine Learning (with Code Examples)**: [Example Application](https://www.dataquest.io/blog/regularization-in-machine-learning/)\n",
    "3. **Polynomial Regression**: [Educational Resource](https://medium.com/analytics-vidhya/polynomial-regression-%EF%B8%8F-e0e20bfbe9d5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Challenge of Overfitting**\n",
    "\n",
    "- **Risk in Flexibility**:  Higher-degree polynomials might fit the training data too precisely, capturing noise rather than the underlying trend, a phenomenon known as overfitting.\n",
    "- **Consequences**: An overfitted model performs poorly on new, unseen data, rendering it less effective for predictive purposes.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Regularization: A Key Solution**\n",
    "\n",
    "Regularization addresses the challenge of overfitting by introducing constraints into the model fitting process. It penalizes the complexity of the model, encouraging simpler models that are more likely to generalize well. <br>\n",
    "One of the possible loss or error functions that considers a penalization term is,\n",
    "$$\n",
    "  \\cal{L}(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( f(\\mathbf{x}_i) - y_i \\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\left( \\mathbf{w}^\\top \\mathbf{x}_i - y_i \\right)^2  + \\frac{\\lambda}{2}\\sum_{i=1}^{d} w_i^2\n",
    "$$\n",
    "$$\n",
    "  \\cal{L}(\\mathbf{w}) = \\frac{1}{2} \\left (\\mathbf{y} - \\mathbf{X} \\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right) + \\frac{\\lambda}{2}\\| \\mathbf{w}\\|^2,\n",
    "$$\n",
    "where $\\| \\mathbf{w}\\|^2 = \\mathbf{w}^\\top\\mathbf{w} = \\sum_{i}^N w_i^2$, and $\\lambda$ is known as the **regularization parameter**.\n",
    "\n",
    "Following the same protocol where we use,\n",
    "$$\n",
    " \\nabla {\\cal L}(\\mathbf{w}) \\Big\\rvert_{\\mathbf{w}^{*}} = 0\n",
    "$$\n",
    "to solve for for the optimal value of $\\mathbf{w}$, we get,\n",
    "$$\n",
    " \\boldsymbol{\\theta}^{*}(\\lambda) = \\left ( \\mathbf{X}^{\\top} \\mathbf{X}  + \\lambda \\mathbb{I}\\right)^{-1} \\mathbf{X}^{\\top}\\mathbf{y}.\n",
    "$$\n",
    "$\\mathbb{I}$ is the identity matrix, a matrix full of zeros except the diagonal values are equal to one.\n",
    "\n",
    "<br>\n",
    "\n",
    "The avobe procedure can also be applied to a polynomial model, \n",
    "$$\n",
    "\\phi(\\mathbf{x}) = [1, x_1, x_2, x_3, \\cdots, x_i x_j, \\cdots, x_i^{ m} x_j^{p}, \\cdots, x_i^{ m} x_j^{p}x_{\\ell}^{r}].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Formula and Implementation**: The optimal parameters for a regularized polynomial model can be computed using the formula:\n",
    "  - $\\boldsymbol{\\theta}^{*}(\\lambda) = \\left ( \\mathbf{\\Phi}(\\mathbf{X})^{\\top} \\mathbf{\\Phi}(\\mathbf{X})  + \\lambda \\mathbb{I}\\right)^{-1} \\mathbf{\\Phi}(\\mathbf{X})^{\\top}\\mathbf{y}$. \n",
    "- **Balancing Act**: The regularization parameter $\\lambda$ plays a crucial role in balancing the model's complexity against its ability to generalize. It determines the extent to which the model's complexity is penalized, impacting both bias and variance.\n",
    "- **The parameter $\\boldsymbol{\\theta} \\$:** In the context of regularization are the coefficients of the model that are adjusted to minimize the penalty imposed by regularization.\n",
    "\n",
    "> **Library:** [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) from `Sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding #\n",
    "* Using the previous code, let's revisit the optimization of the same problem for different values of $\\lambda$.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data over f(x) = sin(x) + x - 1\n",
    "def get_data(N, bool_biased=True):\n",
    "    # This creates an array x of N linearly spaced values between -1 and 1.\n",
    "    x = np.linspace(-1., 1., N)\n",
    "    y = np.sin(.5*x) + x - 1.\n",
    "    # Adds random noise to each y value.\n",
    "    y = y + np.random.uniform(low=0., high=0.5, size=x.shape)\n",
    "    if bool_biased:\n",
    "        X = np.column_stack((np.ones_like(x), x))\n",
    "    else:\n",
    "        X = x[:, None]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, deg):\n",
    "    poly = PolynomialFeatures(deg)\n",
    "    Phi = poly.fit_transform(X)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_model_solver(X, y, deg, l=1E-2):\n",
    "    Phi = polynomial_features(X, deg)\n",
    "    A = Phi.T@Phi + l * np.eye(Phi.shape[1], Phi.shape[1])\n",
    "    w = np.linalg.inv(A)@(Phi.T@y)\n",
    "    # w = linear_model_solver(Phi,y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last's lecture example\n",
    "\n",
    "l = 1E-3 # regularization parameter\n",
    "N = 10 # number of training data\n",
    "\n",
    "X, y = get_data(N, False)\n",
    "x_grid = np.linspace(-1.25, 1.25, 100)[:, None] # prediction grid\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "w_ = []  # to save the number of parameters\n",
    "p_ = np.arange(5, 13, 1, dtype=np.int32)  # polynomial degrees\n",
    "for p in p_:  # loop over different degrees\n",
    "    w = polynomial_model_solver(X, y, p, l)\n",
    "    Phi = polynomial_features(x_grid, p)\n",
    "    y_pred = Phi@w\n",
    "\n",
    "    plt.plot(x_grid, y_pred, label='p=%s' % p)\n",
    "    w_.append(np.pad(w, (0, 13-w.shape[0]),\n",
    "              mode='constant', constant_values=0))\n",
    "plt.scatter(X[:, -1], y, s=75, label='data')\n",
    "plt.title(r'$\\lambda$ = ' + f'%.3f'%(l))\n",
    "plt.legend(fontsize=5)\n",
    "plt.xlabel(r'$x$', fontsize=18)\n",
    "plt.ylabel(r'$f(x)$', fontsize=18)\n",
    "# plt.savefig('Figures/polyfit_2.png',dpi=1800)\n",
    "\n",
    "fig, ax0 = plt.subplots(1, 1)\n",
    "c = ax0.pcolor(np.abs(np.array(w_)), edgecolors='k', linewidths=4)\n",
    "fig.colorbar(c, ax=ax0, label=r'$|w_i|$')\n",
    "ax0.set_xlabel(r'$w_i$')\n",
    "ax0.set_yticks(np.arange(p_.shape[0])+0.5, p_)\n",
    "ax0.set_ylabel('Poly degree')\n",
    "fig.tight_layout()\n",
    "plt.title(r'$\\lambda$ = ' + f'%.3f' % (l))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of the regularization term #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ = np.array([0., 1E-4, 1E-3, 1E-2, 1E-1, 1.])  # lambda\n",
    "w_ = []\n",
    "deg = 12\n",
    "N = 10\n",
    "X,y = get_data(N,False)\n",
    "for l in l_:  # loop over variuos lambda\n",
    "    w = polynomial_model_solver(X, y, deg, l)\n",
    "    w_.append(w)  # list of optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax0 = plt.subplots(1, 1)\n",
    "c = ax0.pcolor(np.abs(np.array(w_)), edgecolors='k', linewidths=4)\n",
    "fig.colorbar(c, ax=ax0, label=r'$|w_i|$')\n",
    "ax0.set_xlabel(r'$w_i$', fontsize=15)\n",
    "theta_ticks = [r'$w_{%s}$' % i for i in range(len(w_[0]))]\n",
    "ax0.set_xticks(np.arange(len(w_[0]))+0.5, theta_ticks, fontsize=10)\n",
    "ax0.set_yticks(np.arange(l_.shape[0])+0.5, l_)\n",
    "ax0.set_ylabel(r'$\\lambda$', fontsize=15)\n",
    "plt.title(f'p = %s, N = %s'%(deg,N))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of determination $R^2$\n",
    "(**homework**)\\\n",
    "$R^{2} = 1-\\frac{SS_{res}}{SS_{tot}}$,\\\n",
    "where,\\\n",
    "$SS_{res} = \\sum_{i} (y_i - f(x_i))^2$\\\n",
    "$SS_{tot} = \\sum_{i} (y_i - \\hat{y})^2$\\\n",
    "$\\hat{y} = \\frac{1}{N}\\sum_i^{N} y_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "def r_square(X,y,l,deg):\n",
    "    \n",
    "    # solve for theta\n",
    "    \n",
    "    # compute ss_res \n",
    "    \n",
    "    # compute ss_tot\n",
    "    \n",
    "    r2 = # \n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation error vs training error # \n",
    "One of the reasons why regularization is key in model fitting is illustrated in the following exercise.\n",
    "1. Split the data into training and validation, N total = 20, N training = 15, N validation = 5. \n",
    "2. Using the validation data, verified with model trained with different values of $\\lambda$ has the lowest validation error. \n",
    "\n",
    "Let's use the root mean square error (RMSE), as the metric for the validation error. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(prediction,target):\n",
    "    diff = prediction - target\n",
    "    mse = np.mean(diff**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "Ntr = 15\n",
    "Nval = 5\n",
    "Xtotal, ytotal = get_data(N,False) \n",
    "\n",
    "\n",
    "# split the data randomly into training and validation\n",
    "i = np.arange(N)\n",
    "i0 = np.random.permutation(i)\n",
    "i0_tr = i0[:Ntr]\n",
    "i0_val = i0[Ntr:]\n",
    "\n",
    "X_tr, y_tr = Xtotal[i0_tr], ytotal[i0_tr]\n",
    "X_val, y_val = Xtotal[i0_val], ytotal[i0_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ = np.array([0., 1E-4, 1E-3, 1E-2, 1E-1, 1.])  # lambda\n",
    "w_ = []\n",
    "deg = 12\n",
    "N = 10\n",
    "X, y = get_data(N, False)\n",
    "rmse_tr_all = []\n",
    "rmse_val_all = []\n",
    "for l in l_:  # loop over variuos lambda\n",
    "    # compute the rmse for training and validation for different values of lambda\n",
    "    w = polynomial_model_solver(X_tr, y_tr,deg,l)\n",
    "    rmse_tr = \n",
    "    rmse_val = \n",
    "    \n",
    "    rmse_tr_all.append(rmse_tr)\n",
    "    rmse_val_all.append(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the rmse validation vs the values of lambda "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem4pb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
