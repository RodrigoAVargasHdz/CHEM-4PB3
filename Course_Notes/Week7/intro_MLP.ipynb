{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open on Google Colab [link](https://colab.research.google.com/github/RodrigoAVargasHdz/CHEM-4PB3/blob/main/Course_Notes/Week7/intro_MLP.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond linear models\n",
    "\n",
    "## What if $\\phi(\\cdot)$ also depends on internal paramters? **non-linear model**,\n",
    "$f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x},\\mathbf{w}') = \\sum_i w_i \\phi_i(\\mathbf{x},\\mathbf{w}')$.\n",
    "\n",
    "Now we also need to optimize the non-linear parameters $\\mathbf{w}'$.\n",
    "\n",
    "**Diagram**\\\n",
    "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/nonLinear_model_diagram.png\"  width=\"350\" height=\"300\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume $\\phi(\\mathbf{x},\\mathbf{w}')$ is another linear model,\\\n",
    "$\\phi(\\mathbf{x},\\mathbf{w}') = \\mathbf{z} = [z_0,z_1,\\cdots,z_\\ell]$, where $\\ell$ is the \"new\" number of features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def f(x):\n",
    " return -(1.4 - 3.0 * x) * torch.sin(18.0 * x)\n",
    "\n",
    "def get_data(n_batch=25):\n",
    "    # X = torch.randn((n_batch,1))\n",
    "    X = torch.distributions.uniform.Uniform(-0.01,1.).sample([n_batch,1]) \n",
    "    y = f(X)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 100),\n",
    "    nn.Linear(100,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,training_iter = 500,n_batch = 25):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    training_iter = 500\n",
    "    n_batch = 25\n",
    "    for itr in range(1,training_iter):\n",
    "\n",
    "        X,y_true = get_data(n_batch)\n",
    "        output = model(X)\n",
    "        loss_val = loss(output,y_true)\n",
    "        \n",
    "        # l_norm = sum(p.pow(2.0).sum()\n",
    "        #               for p in model.parameters())\n",
    "        # l_norm = sum(p.abs().sum()for p in model.parameters())\n",
    "        \n",
    "        loss_val = loss_val # + l2_lambda*l_norm\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if itr%5 == 0 :\n",
    "            print(f'itr = %s, loss = %.4f'%(itr,loss_val.item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = training(model)\n",
    "X_grid = torch.linspace(0.,1.,5000).unsqueeze(1)\n",
    "plt.clf()\n",
    "X,y = get_data(25)\n",
    "print(X.shape,y.shape,model(X).shape)\n",
    "plt.scatter(X.detach().numpy(),y.detach().numpy(),label='Batch')\n",
    "plt.plot(X_grid.detach().numpy(),f(X_grid).detach().numpy(),ls='--',c='k',label=r'$f(x)$')\n",
    "plt.plot(X_grid.detach().numpy(),model(X_grid).detach().numpy(),c='red',label=r'$NN(x)$')\n",
    "plt.ylabel(r'$f(x)$',fontsize=12)\n",
    "plt.xlabel(r'$x$',fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-linear layers\n",
    "\n",
    "**What happens if we consider a linear model of another-linear model (two linear models)?**\n",
    "\n",
    "$$\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) =  \\left ( \\mathbf{x}\\mathbf{W}^\\top_1 \\right )\\mathbf{W}^\\top_2 = \\mathbf{z}\\mathbf{W}^\\top_2\n",
    "$$\n",
    "\n",
    "$f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2})= \\mathbf{W}_2^\\top \\phi(\\mathbf{W}_1, \\mathbf{x})$, where $\\phi(\\mathbf{W}_1, \\mathbf{x}) = \\left ( \\mathbf{x}\\mathbf{W}^\\top_1 \\right )$.\n",
    "\n",
    "\n",
    "\n",
    "<!-- We will use the following notation for the elements of $[\\mathbf{W}_{\\ell}]_{i,j} = w_\\kappa^{\\ell,i}$,\n",
    "* $\\ell$ is the layer index\n",
    "* $\\kappa$ is the *input-feature* index\n",
    "* $i$ is the *output-feature* index\n",
    "  \n",
    "```nn.Linear()``` is a linear transformation of the following way,\n",
    "$$ \\mathbf{x}\\mathbf{W}_1^\\top = \\begin{bmatrix}\n",
    " x_0,& \\cdots, & x_d \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    " &  &  \\\\\n",
    "\\mathbf{w}_1, & \\cdots, & \n",
    "\\mathbf{w}_\\ell \\\\\n",
    " &  &  \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\mathbf{x}\\mathbf{W}_1^\\top = \\begin{bmatrix}\n",
    " x_0,& \\cdots, & x_d \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "w^{1,1}_0& w^{1,j}_0 & w^{1,\\ell}_0 \\\\\n",
    "w^{1,1}_i & w^{1,j}_i & w^{1,\\ell}_i \\\\\n",
    "w^{1,1}_d & w^{1,j}_d & w^{1,\\ell}_d \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\mathbf{x}\\mathbf{W}^\\top = \\begin{bmatrix}\n",
    " \\mathbf{w}_1^\\top \\mathbf{x},& \\cdots, & \\mathbf{w}_i^\\top \\mathbf{x}, & \\cdots, & \\mathbf{w}_\\ell^\\top \\mathbf{x} \\\\\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where $\\mathbf{z}$ is a vector with $\\ell$ entries. \n",
    "\n",
    " \n",
    "Let's assume that $\\mathbf{W}_2^\\top$ is a $(\\ell,1)$ matix and  $\\mathbf{W}_1^\\top$ is a $(d,\\ell)$ matrix.\n",
    "\n",
    "$$\n",
    "\\mathbf{z}\\mathbf{W}_2^\\top = \\begin{bmatrix}\n",
    " z_0,& \\cdots, & z_\\ell \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{0}^{2,1} \\\\\n",
    "w_{i}^{2,1} \\\\\n",
    "w_{\\ell}^{2,1} \\\\ \n",
    "\\end{bmatrix} = \\sum_{i}^{\\ell} w_{i}^{2,1}  \\; z_{i}.\n",
    "$$\n",
    "* the $2$ in $w^{2,1}$ means the index of the **second** linear model.\n",
    "\n",
    "From the above equation we can obtain an expression for $\\mathbf{z}$,\n",
    "$$\n",
    "\\mathbf{z} = \\mathbf{x}\\mathbf{W}_1^\\top = \\begin{bmatrix}\n",
    " \\mathbf{w}_1^\\top \\mathbf{x},& \\cdots, & \\mathbf{w}_i^\\top \\mathbf{x}, & \\cdots, & \\mathbf{w}_\\ell^\\top \\mathbf{x} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{z} = \\begin{bmatrix}\n",
    " \\sum_jw^{1,1}_j \\; x_j, & \\sum_j w^{1,2}_j \\; x_j, & \\cdots, & \\sum_jw^{1,\\ell-1}_j \\; x_j, &  \\sum_jw^{1,\\ell}_j \\; x_j \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Combining all of the above we get,\n",
    "$$\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\mathbf{z}\\mathbf{W}^\\top_2 = \\sum_i^\\ell w_{i,2} \\; z_i  \\\\\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\sum_i^\\ell w_{i}^{2,1} \\; \\left ( \\mathbf{w}_i^\\top \\mathbf{x} \\right )\\\\\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\sum_i^\\ell w_{i}^{2,1} \\; \\left ( \\sum_j^d w_{j}^{1,i} \\; x_j \\right ) \\\\\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\sum_i^\\ell  \\; \\left ( \\sum_j^d w_{i}^{2,1} w_{j}^{1,i} \\; x_j \\right ) \\\\\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\sum_i^\\ell  \\; \\left ( \\sum_j^d \\omega^{i}_{j} \\; x_j \\right ) \\\\\n",
    "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) =  \\sum_j^d \\omega^{1}_{j}  \\; x_j = \\mathbf{\\omega}^\\top \\mathbf{x} \\\\\n",
    "$$\n",
    "\n",
    "$f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2})$ is another linear model. -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearies\n",
    "\n",
    "We can make $\\phi(\\mathbf{W}_\\ell, \\mathbf{x})$ a non-linear transformation using an element-wise non-linear function.\n",
    "*  element-wise non-linear function -> **activation function**\n",
    "\n",
    "## activation functions\n",
    "\n",
    "* hyperbolic tangent\n",
    "$$\n",
    "tanh(x) = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)}\n",
    "$$\n",
    "\n",
    "* Sigmoid\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{1}{1+ \\exp(-x)}\n",
    "$$\n",
    "\n",
    "* ReLU\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0,x)\n",
    "$$\n",
    "\n",
    "* Leaky RLU\n",
    "$$\n",
    "\\text{LeakyReLU}(x) = \\max(0,x) + \\beta*\\min(0,x)\n",
    "$$\n",
    "\n",
    "* SiLU\n",
    "$$\n",
    "\\text{SiLU}(x) = x * \\sigma(x)\\\\\n",
    "\\sigma(x) = \\frac{1}{1+\\exp(-x)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5,5,1000)\n",
    "\n",
    "act_tanh = nn.Tanh()\n",
    "y_tanh = act_tanh(x)\n",
    "\n",
    "act_sigmoid = nn.Sigmoid()\n",
    "y_sigmoid = act_sigmoid(x)\n",
    "\n",
    "act_relu = nn.ReLU()\n",
    "y_relu = act_relu(x)\n",
    "\n",
    "act_lrelu = nn.LeakyReLU(0.1)\n",
    "y_lrelu = act_lrelu(x)\n",
    "\n",
    "act_silu = nn.SiLU()\n",
    "y_silu = act_silu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnp = x.detach().numpy()\n",
    "plt.plot(xnp,y_tanh.detach().numpy(),label='Tanh')\n",
    "plt.plot(xnp,y_sigmoid.detach().numpy(),label='Sigmoid')\n",
    "plt.plot(xnp,y_relu.detach().numpy(),label='ReLU')\n",
    "plt.plot(xnp,y_lrelu.detach().numpy(),label='Leaky ReLU')\n",
    "plt.plot(xnp,y_silu.detach().numpy(),label='SiLU')\n",
    "plt.xlabel('x',fontsize=15)\n",
    "plt.ylabel('activation function',fontsize=15)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose one of the above activation functions and use it in your linear model.\n",
    "\n",
    "**Diagram**\\\n",
    "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/MLP_diagram.png\"  width=\"400\" height=\"300\">\n",
    "\n",
    "<!-- '''python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 100),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(100, 1)\n",
    ")\n",
    "''' -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "# define a model\n",
    "\n",
    "model = training(model)\n",
    "X_grid = torch.linspace(0., 1., 5000).unsqueeze(1)\n",
    "plt.clf()\n",
    "X, y = get_data(25)\n",
    "print(X.shape, y.shape, model(X).shape)\n",
    "plt.scatter(X.detach().numpy(), y.detach().numpy(), label='Batch')\n",
    "plt.plot(X_grid.detach().numpy(), f(X_grid).detach().numpy(),\n",
    "         ls='--', c='k', label=r'$f(x)$')\n",
    "plt.plot(X_grid.detach().numpy(), model(\n",
    "    X_grid).detach().numpy(), c='red', label=r'$NN(x)$')\n",
    "plt.ylabel(r'$f(x)$', fontsize=12)\n",
    "plt.xlabel(r'$x$', fontsize=12)\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in a small groups and discuss the following.\n",
    "1. How many layers we need?\n",
    "2. What is the *best* activation function?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra\n",
    "\n",
    "Go to the following [link](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.03345&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) and try to solve all the different tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem4pb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bd1d19d093f1d1d5dd6cf1becc7a1cffecc3235fce6b74420ac04427a66c9c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
