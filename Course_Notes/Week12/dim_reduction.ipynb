{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdkit-pypi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw, rdMolDescriptors, rdDistGeom, rdMolTransforms, QED\n",
    "from rdkit.Chem.Scaffolds.MurckoScaffold import GetScaffoldForMol\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://moleculenet.org/datasets-1\n",
    "# data = pd.read_csv('/Users/ravh011/Documents/McMaster/Courses/CHEM_4PB3_2022/Course_Notes/data/HIV.csv')\n",
    "data_url = \"https://github.com/RodrigoAVargasHdz/CHEM-4PB3/raw/main/Course_Notes/data/HIV.csv\"\n",
    "data = pd.read_csv(data_url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['HIV_active']\n",
    "y0 = y==0\n",
    "y1 = y==1\n",
    "y0_class = data['HIV_active'][y0]\n",
    "y1_class = data['HIV_active'][y1]\n",
    "print(y0_class.shape,y1_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprints(m_smiles, radius=2, nbits=2048):\n",
    "  m = Chem.MolFromSmiles(m_smiles)\n",
    "  m = Chem.AddHs(m)\n",
    "  m_fingerprints = AllChem.GetMorganFingerprintAsBitVect(\n",
    "      m, radius=radius, nBits=nbits)\n",
    "  return np.asarray(m_fingerprints)\n",
    "\n",
    "def get_classical_md(m):\n",
    "    canon_smiles = AllChem.MolToSmiles(m, canonical=True)\n",
    "    # number of H-bond acceptors for a molecule\n",
    "    hba = rdMolDescriptors.CalcNumHBA(m)\n",
    "\n",
    "    # number of H-bond donors for a molecule\n",
    "    hbd = rdMolDescriptors.CalcNumHBD(m)\n",
    "\n",
    "    # number of rings for a molecule\n",
    "    nrings = rdMolDescriptors.CalcNumRings(m)\n",
    "\n",
    "    # number of rotatable bonds for a molecule\n",
    "    rtb = rdMolDescriptors.CalcNumRotatableBonds(m)\n",
    "\n",
    "    #  topological polar surface area (TPSA) of a molecule (used medicinal chemistry metric for the optimization of a drug's ability to permeate cells.)\n",
    "    psa = rdMolDescriptors.CalcTPSA(m)\n",
    "\n",
    "    # logP and mr from https://pubs.acs.org/doi/10.1021/ci990307l:\n",
    "    # logP ->  water partition coefficient as measure of lipophilicity\n",
    "    # MR -> molar refractivity\n",
    "    logp, mr = rdMolDescriptors.CalcCrippenDescriptors(m)\n",
    "\n",
    "    # molecular weight\n",
    "    mw = rdMolDescriptors._CalcMolWt(m)\n",
    "\n",
    "    # Csp3: fraction of sp3 carbons\n",
    "    csp3 = rdMolDescriptors.CalcFractionCSP3(m)\n",
    "    \n",
    "    # fraction of atoms belonging to Murcko framework\n",
    "    # number of heavy atoms for a molecule\n",
    "    fmf = GetScaffoldForMol(m).GetNumHeavyAtoms() / m.GetNumHeavyAtoms()\n",
    "    hac = m.GetNumHeavyAtoms()\n",
    "\n",
    "    # max_ring_size: maximum ring size in a molecule\n",
    "    max_ring_size = len(max(m.GetRingInfo().AtomRings(), key=len, default=()))\n",
    "\n",
    "    # QED: quantitative estimate of drug-likeness (https://www.rdkit.org/docs/source/rdkit.Chem.QED.html)\n",
    "    qed = QED.qed(m)\n",
    "    \n",
    "    # ChiralCenters: number of chiral centers (assigned and unassigned)\n",
    "    n_chiral_centers = len(Chem.FindMolChiralCenters(m, includeUnassigned=True))\n",
    "    \n",
    "    r = np.array([hba,hbd,nrings,rtb,logp,mr,mw,csp3,fmf,qed,hac,n_chiral_centers,max_ring_size,psa])\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = {}\n",
    "smiles_all = data['smiles']\n",
    "for i, smi in enumerate(smiles_all):\n",
    "    # m = AllChem.MolFromSmiles(smi)\n",
    "    # r = get_fingerprints(smi)\n",
    "    r = get_classical_md(smi)\n",
    "    D.update({i: r})\n",
    "    \n",
    "print(D)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "So far we studied how to use \"label\" data in order to parametrize functions to predict target tasks,\n",
    "$$\n",
    "\\mathbf{y} = f(\\mathbf{x};\\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "To achieve this learning we rely on error functions that compare the prediction between our model and the ground truth values,\n",
    "$$\n",
    "{\\cal L}(\\boldsymbol{\\theta}) = \\|\\hat{\\mathbf{y}} - f(\\mathbf{X};\\boldsymbol{\\theta})\\|_2\\\\\n",
    "{\\cal L}(\\boldsymbol{\\theta}) = \\|\\hat{\\mathbf{y}} - f(\\mathbf{X};\\boldsymbol{\\theta})\\|_2 + \\lambda \\|\\boldsymbol{\\theta}\\|_2 \\\\\n",
    "{\\cal L}(\\boldsymbol{\\theta}) = |\\hat{\\mathbf{y}} - f(\\mathbf{X};\\boldsymbol{\\theta})|,\n",
    "$$\n",
    "or other loss functions that depend on $\\hat{\\mathbf{y}}$.\n",
    "\n",
    "What if $\\hat{\\mathbf{y}}$ is unknown? \n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "In unsupervised learning the data only consists on a set of input vectors $\\{\\mathbf{x}_i\\}_i^N$ without any corresponding target values. Unsupervised learning algorithms can be described using three main approaches,\n",
    "1. **clustering**, discover groups of similar examples within the data.\n",
    "2. **data generation**, determine the distribution of data within the input space.\n",
    "3. **dimensionality reduction**, project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization.\n",
    "\n",
    "We will only focus on **dimensionality reduction**, and cover two of the must common algorithms,\n",
    "1. Principal component analysis (PCA)\n",
    "2. t-distributed stochastic neighbor embedding (t-SNE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (auto-encoders) Dimensionality reduction\n",
    "Since the only source of information we have is the training data, one possible error function that we could use is,\n",
    "$$\n",
    "{\\cal L}(\\boldsymbol{\\theta}) = \\| \\mathbf{X} - f(\\mathbf{Z};\\boldsymbol{\\theta})\\|_2 = \\sum_i^N (\\mathbf{x}_i - f(\\mathbf{z}_i,\\boldsymbol{\\theta}))^2,\n",
    "$$\n",
    "where $\\mathbf{z}_i$ is some \"encoded\" version of $\\mathbf{x}_i$  and $f(\\cdot)$ is the function that transforms back to the original feature representation. Usually, $\\mathbf{z}$'s dimension is smaller than $\\mathbf{x}$.  \n",
    "\n",
    "* What about $\\mathbf{z}$? \\\n",
    "We can also learn this encoding using another function, $\\mathbf{z} = g(\\mathbf{x};\\boldsymbol{\\theta})$.\n",
    "\n",
    "$$\n",
    "{\\cal L}(\\boldsymbol{\\theta}) = \\| \\mathbf{X} - f(\\mathbf{Z};\\boldsymbol{\\theta}_d)\\|_2\\\\\n",
    "{\\cal L}(\\boldsymbol{\\theta}_e,\\boldsymbol{\\theta}_d) = \\| \\mathbf{X} - f(g(\\mathbf{X};\\boldsymbol{\\theta}_e),\\boldsymbol{\\theta}_d)\\|_2\\\n",
    "$$\n",
    "\n",
    "<!-- $f(g(\\mathbf{X};\\boldsymbol{\\theta}_d),\\boldsymbol{\\theta}_e)$\\ -->\n",
    "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/autoencoder.png\"  width=\"250\" height=\"200\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe you have all the tools to construct such models (auto-encoders) in torch. \n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,x_dim,latent_dim):\n",
    "        super(MolecularVAE, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.e = nn.Sequential(\n",
    "            nn.Linear(self.x_dim, self.latent_dim),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "            \n",
    "        self.d = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.x_dim),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "\n",
    "    def encoder(self,x):\n",
    "        z = self.e(x)\n",
    "        return z     \n",
    "    def decoder(self,x):\n",
    "        z = self.d(x)\n",
    "        return z \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is by far the most used technique to compress data into lower dimensions, ([review paper](https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202)).\\\n",
    "I will try to motivate PCA again under the perspective of, \n",
    "$$\n",
    "{\\cal L}(\\boldsymbol{\\theta}) =  \\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_2 \n",
    "$$\n",
    "where $\\tilde{\\mathbf{X}}$ is the reconstructed $\\mathbf{X}$ using the PCA model.\n",
    "\n",
    "In PCA, each reconstructed point can be computed as, \n",
    "$$\n",
    "\\tilde{\\mathbf{x}}_i = \\sum_i^M z_{ni}\\mathbf{u}_i + \\sum_{i=M+1}^D b_{i}\\mathbf{u}_i,\n",
    "$$\n",
    "where all $\\mathbf{u}_i$  form an orthonormal set of D-dimensional basis vectors ($\\mathbf{u}_j^\\top\\mathbf{u}_i = \\delta_{ij}$).\n",
    "The parameters of PCA are all $z_{ni}$, $b_{i}$, and $\\mathbf{u}_i$, and we can optimize them so $\\tilde{\\mathbf{x}}_i \\approx \\mathbf{x}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function of PCA is also a square distance between original and reconstructed,\n",
    "$$\n",
    "{\\cal L}(\\boldsymbol{\\theta}) =  \\frac{1}{N} \\sum_i^N (\\mathbf{x}_i - \\tilde{\\mathbf{x}}_i)^2\n",
    "$$\n",
    "First, let's consider the average point $\\overline{\\mathbf{x}} = \\frac{1}{N}\\sum_i^N\\mathbf{x}_i$,\n",
    "$$\n",
    "{\\cal L}(\\boldsymbol{\\theta}) =  \\frac{1}{N} \\sum_n^N \\sum_{i=M+1}^D (\\mathbf{x}_n^\\top\\mathbf{u}_i - \\overline{\\mathbf{x}}^\\top\\mathbf{u}_i)^2 = \\sum_{i=M+1}^D \\mathbf{u}_i^\\top\\mathbf{S}\\mathbf{u}_i\n",
    "$$\n",
    "\n",
    "Remember $\\mathbf{u}_i$ must be orthonormal, so $\\mathbf{u}_i^\\top\\mathbf{S}\\mathbf{u}_i$ can be solved using the **singular value decomposition (SVD)** of matrix **S**.\\\n",
    "**S** is the data covariance matrix defined by\n",
    "$$\n",
    "\\mathbf{S} = \\frac{1}{N} \\sum_n^N(\\mathbf{x_n}-\\overline{\\mathbf{x}})(\\mathbf{x_n}-\\overline{\\mathbf{x}})^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "<!-- By setting,\n",
    "$$\n",
    "\\frac{\\partial {\\cal L}}{\\partial z_{ni}} = 0, \\text{ we get, } z_{ni} = \\mathbf{x}_i^\\top \\mathbf{u}_i\\\\\n",
    "\\frac{\\partial {\\cal L}}{\\partial b_j} = 0, \\text{ we get, } b_j = \\overline{\\mathbf{x}}^\\top \\mathbf{u}_j\n",
    "$$\n",
    "$\\overline{\\mathbf{x}} = \\frac{1}{N}\\sum_i^N\\mathbf{x}_i$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Sklearn https://github.com/scikit-learn/scikit-learn/blob/9aaed4987/sklearn/decomposition/_pca.py#L118\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#get data\n",
    "X = np.array([D[key] for i, key in enumerate(D)])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(ncols=2)\n",
    "axs[0].plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "axs[0].set_xlabel('Number of components')\n",
    "axs[0].set_ylabel('Explained variance')\n",
    "\n",
    "axs[1].plot(np.arange(len(pca.singular_values_)),np.array(pca.singular_values_))\n",
    "axs[1].set_ylabel(f'$\\lambda_i$')\n",
    "axs[1].set_xlabel('Index')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_comp = pca.components_[:n_components]\n",
    "\n",
    "for i,x in enumerate(pca.components_[:3]):\n",
    "    plt.plot(np.arange(x.shape[0]),x,label='%i'%(i))\n",
    "    # print(x)\n",
    "plt.ylabel('Value of the PCA component')\n",
    "plt.xlabel('Finger Print index')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "X_r = pca.transform(X)\n",
    "\n",
    "y = data['HIV_active']\n",
    "\n",
    "y_c = ['blue' if yi == 0 else 'red' for yi in y]\n",
    "\n",
    "# plt.scatter(X_r[:,0],X_r[:,1],color=y_c)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter3D(X_r[:, 0], X_r[:, 1], X_r[:, 2], c=y_c,s=2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Neighbor Embedding (t-SNE)\n",
    "[paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n",
    "\n",
    "Notation,\n",
    "* $p_{j|i}$, probability that point $x_i$ would pick $x_j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at $x_i$.\n",
    "* $y_i$ is a low dimensional representation of $x_i$.\n",
    "* $q_{i|j}$ is similar to $p_{j|i} just in the low dimensional representation.\n",
    "\n",
    "If the map points $y_i$ and $y_j$ correctly model the similarity between the high-dimensional data-\n",
    "points $x_i$ and $x_j$, the conditional probabilities $p_{j|i}$ and $q_{i|j}$ will be equal. \n",
    "\n",
    "In t-SNE, the loss function is motivated by similarity between the $p_{j|i}$ and $q_{i|j}$ distributions,\n",
    "$$\n",
    "{\\cal L} = \\sum_i KL(P_i||Q_i) = \\sum_i\\sum_j p_{j|i}\\log\\frac{p_{j|i}}{q_{j|i}}\n",
    "$$\n",
    "\n",
    "The similarity of map point  $y_j$  to map point  $y_i$ is given by,\n",
    "$$\n",
    "q_{i|j} = \\frac{\\exp(-\\|y_j-y_i\\|^2)}{\\sum_{k\\neq i}\\exp(-\\|y_j-y_i\\|^2)}\n",
    "$$\n",
    "\n",
    "\n",
    "SNE minimizes the sum of Kullback-Leibler (KL) divergences over all data points using a gradient descent\n",
    "method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "t_sne_model = manifold.TSNE(\n",
    "    n_components=3,\n",
    "    perplexity=30,\n",
    "    init=\"random\",\n",
    "    n_iter=250,\n",
    "    random_state=0,\n",
    ")\n",
    "X_t_sne = t_sne_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sne_model = manifold.TSNE(\n",
    "    n_components=3,\n",
    "    perplexity=30,\n",
    "    init=\"random\",\n",
    "    n_iter=250,\n",
    "    random_state=0,\n",
    ")\n",
    "X_t_sne = t_sne_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_t_sne.shape)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter3D(X_t_sne[:, 0], X_t_sne[:, 1], X_t_sne[:, 2], c=y_c,s=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More embeddings for data\n",
    "\n",
    "[Sklearn](https://scikit-learn.org/stable/modules/manifold.html#spectral-embedding) has a super nice tutorial on other data embedding methods. All depend on different heuristics on distance between pair of data points, \n",
    "$$\n",
    "{\\cal L} \\approx \\sum_{i>j}d_{ij}(\\mathbf{X}) - d_{ij}(\\mathbf{\\tilde{X}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem4pb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
