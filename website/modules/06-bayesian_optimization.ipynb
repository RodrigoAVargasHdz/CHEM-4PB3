{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "12d67c1d",
      "metadata": {
        "id": "12d67c1d"
      },
      "source": [
        "# Bayesian Optimization\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/06-atomic_simulation_environment.ipynb)\n",
        "\n",
        "**References:**\n",
        "1. **Chapters 1-3**: [Bayesian Optimization](https://bayesoptbook.com/book/bayesoptbook.pdf), R. Garnett\n",
        "2. **Chapters 6**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
        "3. **Chapter 2**:  [Gaussian Processes for Machine Learning](https://direct.mit.edu/books/oa-monograph-pdf/2514321/book_9780262256834.pdf), C. E. Rasmussen, C. K. I. Williams\n",
        "4. **Chapter 4**: [Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198)\n",
        "5. **Chapter 6**: [Probabilistic Machine Learning: An Introduction, K. P. Murphy.](https://probml.github.io/pml-book/book1.html)\n",
        "6. [**The Kernel Cookbook**](https://www.cs.toronto.edu/~duvenaud/cookbook/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e81348",
      "metadata": {
        "id": "73e81348"
      },
      "source": [
        "# Optimization without Gradients\n",
        "\n",
        "During the course, we saw that many scientific problems can be recast as an optimization problem,\n",
        "$$\n",
        "\\mathbf{x}^* = \\arg\\min f(\\mathbf{x})\n",
        "$$\n",
        "where $\\mathbf{x}^*$ is the **minimizer** of the function $f$. <br>\n",
        "We have solved this problem using gradient-based methods, where at each step we used the local information of the gradient to move \"towards\" the minimizer of $f$, using\n",
        "$$\n",
        "\\mathbf{x}^*_{t+1} = \\mathbf{x}^*_{t} - \\eta \\nabla_{\\mathbf{x}}f.\n",
        "$$\n",
        "These style of methods have been successful in scientific frameworks where $f$ is differentiable, or its gradient can be easily estimated. However, for other systems where $f$ is a **black box** function, gradient-based optimization is unfeasible.\n",
        "\n",
        "## Black Box Function\n",
        "* A black box function is a system, algorithm, or piece of code where only the inputs and outputs are visible, while the internal logic, mechanisms, or code structure are hidden or unknown.\n",
        "\n",
        "```\n",
        "x ∈ ℝ^d   ─────▶   [   BLACK BOX  f(x)   ]   ─────▶   y = f(x)\n",
        "(parameters)                                   (objective value)\n",
        "```\n",
        "\n",
        "*  Expensive (minutes–days)\n",
        "*  No gradients available  \n",
        "*  Noisy observations  <br>\n",
        "\n",
        "\n",
        "> In **Bayesian Optimization**, we assume we can query the function, but we cannot inspect its internals. <br>\n",
        "> We cannot differentiate it analytically, and each evaluation may cost minutes, hours, or even days. <br>\n",
        "> Therefore, we must be strategic about where to sample next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4710c6d4",
      "metadata": {
        "id": "4710c6d4"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}