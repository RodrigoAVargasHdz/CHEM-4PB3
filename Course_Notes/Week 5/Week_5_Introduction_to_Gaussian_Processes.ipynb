{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm3J4-R14Srt"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/RodrigoAVargasHdz/CHEM-4PB3/blob/w2024/Course_Notes/Week%205/Week_5_Introduction_to_Gaussian_Processes.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# **Week 5 - Introduction to Gaussian Processes**\n",
        "\n",
        "## **Abstract**\n",
        "\n",
        "1. **Understanding Gaussian Processes in Regression**: The Gaussian Process uses predictions that are made by averaging over all possible functions according to their posterior probabilities. Using Prediction Distribution, Mean Function, and the Covariance function.\n",
        "\n",
        "2. **Introduction to Kernels**: A kernel is a function used to compute the similarity or \"closeness\" between data points in a high-dimensional space. It encodes our assumptions about the function. Common choices include the Radial Basis Function (RBF) kernel.\n",
        "\n",
        "3. **Fitting the Gaussian Processes to N-N**: Applying the Gaussian Processes and a Radial Basis Function (RBF) kernel to fit a Potential Energy Surface (PES) to N-N.\n",
        "\n",
        "\n",
        "\n",
        ">## **References: Essential Resources for Further Learning**\n",
        ">\n",
        ">- **Gaussian Processes**: [Official Documentation](https://scikit-learn.org/stable/modules/gaussian_process.html)\n",
        "- **A Visual Exploration of Gaussian Processes**: [Online Course](https://distill.pub/2019/visual-exploration-gaussian-processes/)\n",
        "- **A tutorial on Gaussian process regression: Modelling, exploring, and exploiting unknown functions**: [Journal Article](https://www.biorxiv.org/content/10.1101/095190v3)\n",
        "- **Assessing Gaussian Process Regression To Represent High-Dimensional Potential Energy Surfaces**: [Research Paper](https://pubs.acs.org/doi/10.1021/acs.jctc.8b00298)\n",
        "\n",
        "\n",
        "\n",
        "Feel free to explore these resources to deepen your understanding of data visualization, data management, and computational tools in Chemistry.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtKzKJHy9Vl7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZvjRHE74X_3"
      },
      "source": [
        "## **Gaussian Processes in Regression**\n",
        "\n",
        "Building off Bayes' Theorem in Regression, the Gaussian Processes in Regression\n",
        "\n",
        "- **Gaussian Process Definition**:\n",
        "  - A Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. In the context of regression, a GP can be used to define a distribution over functions.\n",
        "  - A GP is fully specified by its mean function, $\\ m(x) $, and covariance function, $\\ k(x, x') $, such that:\n",
        "  \n",
        "    $$\n",
        "    \\ f(x) \\sim \\{GP}(m(x), k(x, x'))\n",
        "    $$\n",
        "  - Where:\n",
        "    - $f(x)$ is a random function that is distributed according to the Gaussian process.\n",
        "    - $m(x)$ is the mean function, often assumed to be zero in the absence of prior knowledge.\n",
        "    - $k(x, x')$ is the covariance function, also known as the kernel, which defines the shape of the function space and the smoothness of the functions.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Gaussian Process Regression Visualization](https://simple-complexities.github.io/assets/images/gaussian_process_regression.gif) <br>\n",
        "[**Figure 1 - Gaussian Process Regression Visualization. Image by Ameya Daigavane**](https://simple-complexities.github.io/optimization/gaussian/processes/bayesian/2020/03/16/gaussian-processes.html)\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Applying Gaussian Processes to Regression**\n",
        "\n",
        "In applying GP to regression, we follow these steps:\n",
        "\n",
        "- **Function Space Perspective**:\n",
        "  - Instead of finding the best parameters for a specific function (as in polynomial regression), GP regression considers a space of possible functions that could explain the data.\n",
        "  - The GP prior expresses our belief about the space of functions before observing any data.\n",
        "\n",
        "- **Incorporating Observations**:\n",
        "  - Upon observing data, the GP posterior is updated to reflect the likelihood of these functions given the observed data.\n",
        "  - This update is done using the kernel to measure similarities between points, thus encoding our assumptions about the function's smoothness and variability.\n",
        "\n",
        "- **Prediction and Uncertainty**:\n",
        "  - Predictions are made by averaging over all possible functions according to their posterior probabilities. This results in not just a single prediction for each test point but also a measure of uncertainty (typically given by the variance of the predictive distribution).\n",
        "\n",
        "- **Flexibility of Kernels**:\n",
        "  - The choice of kernel is crucial in GP regression. It encodes our assumptions about the function. Common choices include the Radial Basis Function (RBF) kernel for smooth functions, the Matérn kernel for rough functions, and periodic kernels for repeating patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y8ba-cG4cYl"
      },
      "source": [
        "## **Potential Energy Surface (PES)**\n",
        "\n",
        "A **Potential Energy Surface** **(PES)** is a multidimensional surface that represents the energy of a system, typically of a molecule, as a function of the positions of its nuclei. It is crucial for understanding molecular dynamics and reactions, as it depicts how the potential energy changes with the molecular geometry and guiding the pathways of chemical transformations.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Potential Energy Surface (PES) Visualization](https://www.researchgate.net/profile/Stanislaw-Waclawek/publication/351126865/figure/fig1/AS:1017564494848001@1619617689779/a-Model-potential-energy-surface-showing-an-example-of-a-reaction-path-minima-and.jpg) <br>\n",
        "[**Figure 2 - Potential Energy Surface (PES) Visualization. Image by Stanisław Wacławek**](https://www.researchgate.net/figure/a-Model-potential-energy-surface-showing-an-example-of-a-reaction-path-minima-and_fig1_351126865)\n",
        "\n",
        "<br>\n",
        "\n",
        "> For this example, we'll focus on **N-N**, and its PES.\n",
        "\n",
        "```python\n",
        "from pyscf import gto, scf, mcscf, dft\n",
        "import numpy as np\n",
        "\n",
        "def get_energy(r):\n",
        "  # r  is the N-N distance in the z-axis\n",
        "  mol = gto.M(atom='N 0 0 0; N 0 0 %.3f'%(r), basis='6-311g')\n",
        "  mf = dft.RKS(mol, xc=\"b3lyp\").density_fit().run()\n",
        "  energy = mf.kernel()\n",
        "  return energy\n",
        "```\n",
        "\n",
        "> The data was collected and stored using this code snippet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-qu7Pm114NAe",
        "outputId": "c9be22f6-f0a0-4359-8e84-2aa8988f1bf7"
      },
      "outputs": [],
      "source": [
        "data_url = \"https://github.com/RodrigoAVargasHdz/CHEM-4PB3/raw/main/Course_Notes/data/PES_N2\"\n",
        "data = pd.read_csv(data_url)\n",
        "\n",
        "plt.figure(figsize=(11,7))\n",
        "plt.plot(data['R'], data['Energy'],lw=3.)\n",
        "plt.title('N2 Potential energy surface',fontsize=17)\n",
        "plt.xlabel(r'R [$\\AA$]',fontsize=20)\n",
        "plt.ylabel('Energy',fontsize=20)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrnRBI2C4jKz"
      },
      "source": [
        "### **Gaussian Process Regression Formulas**\n",
        "\n",
        "In Gaussian Process (GP) regression, we model the predictions for new inputs as a normal distribution with mean and variance given by the following formulas:\n",
        "\n",
        "1. **Prediction Distribution**:\n",
        "   The predicted outputs for new input data points are assumed to follow a normal distribution, denoted as $ y_* $.\n",
        "   $$ y_* \\sim \\ N (\\mu(X_*), \\sigma(X_*)) $$\n",
        "\n",
        "2. **Mean Function $\\mu(X_*)$**:\n",
        "   This function represents the expected value of the predictions for the new input points.\n",
        "   $$ \\mu(X_*) = k(X_*, X)^T [K(X, X) + \\sigma_n^2I]^{-1}y $$\n",
        "\n",
        "3. **Covariance Function $\\sigma(X_*)$**:\n",
        "   This function denotes the uncertainty or variance in the predictions for the new input points.\n",
        "   $$ \\sigma(X_*) = k(X_*, X_*) - k(X_*, X)^T [K(X, X) + \\sigma_n^2I]^{-1}k(X_*, X) $$\n",
        "\n",
        "- Here, $k(X_*, X)$ is the kernel function evaluated between the new input points $X_*$ and the training input points $X$, capturing the covariance between them.\n",
        "- $K(X, X)$ is the kernel matrix evaluated on the training input points, representing the covariance among the training points themselves.\n",
        "- $\\sigma_n^2$ is the variance of the noise in the data, and $I$ is the identity matrix.\n",
        "- $y$ is the vector of observed target values from the training set.\n",
        "\n",
        "The GP model thus provides a distribution over possible functions that fit the observed data, and we use this distribution to make predictions with associated uncertainties.\n",
        "\n",
        "\n",
        "![Visualizing GP over Time](https://raw.githubusercontent.com/jwangjie/Gaussian-Process-be-comfortable-using-it/master/img/gpr_animation_wide.gif) <br>\n",
        "[**Figure 3 - Visualizing the Gaussian Process. Image by Jie Wang**](https://paperswithcode.com/paper/an-intuitive-tutorial-to-gaussian-processes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfMenrW1Aejp"
      },
      "source": [
        "### **Introduction to Kernels in Machine Learning**\n",
        "\n",
        "A **kernel** is a function used to compute the **similarity or \"closeness\"** between data points in a **high-dimensional space**. Kernels are a fundamental component in various algorithms, such as Gaussian Processes (GPs). They allow us to measure similarities in input space without explicitly mapping data to a high-dimensional feature space.\n",
        "\n",
        "1. **Kernel Definition**:\n",
        "   A kernel function, typically denoted as \\( k(x, x') \\), takes two input vectors $x$ and \\( x' \\) and returns a scalar representing the similarity between them.\n",
        "   $$ k(x, x') : \\ X \\times \\ X \\rightarrow \\mathbb{R} $$\n",
        "\n",
        "2. **Kernel Properties**:\n",
        "   - **Symmetry**: $$k(x, x') = k(x', x)$$\n",
        "\n",
        "\n",
        "3. **Common Kernels**:\n",
        "   - **Linear Kernel**: Measures the linear correlation between data points.\n",
        "   $$ k(x, x') = x^T x' $$\n",
        "   - **Polynomial Kernel**: Introduces polynomial nonlinearity into the similarity computation.\n",
        "   $$ k(x, x') = (x^T x' + c)^d $$\n",
        "   - **Radial Basis Function (RBF) or Gaussian Kernel**: Measures similarity based on the distance between points in a Gaussian distribution manner.\n",
        "   $$ k_{\\text{RBF}}(x_i, x_j) = \\exp \\left( -\\frac{(x_i - x_j)^2}{2\\ell^2} \\right) $$\n",
        "\n",
        "Kernels allow us to apply algorithms effectively in a high-dimensional space without incurring the computational cost of actually working in that space, a concept known as the \"kernel trick\". This makes them a powerful tool for non-linear classification and regression tasks.\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIe6BJ8SEk76"
      },
      "source": [
        "## **Fitting a Gaussian Process on an N-N Potential Energy Surface (PES)**\n",
        "\n",
        "> Using the Gaussian Process and Radial Basis Function (RBF), let's create a PES for N-N."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKhxeGhTFIvB"
      },
      "outputs": [],
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, DotProduct,\\\n",
        "    RBF, WhiteKernel, Matern, ExpSineSquared, RationalQuadratic\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhKZH1sjFKFQ"
      },
      "outputs": [],
      "source": [
        "# select random data\n",
        "X,y = data['R'].to_numpy(), data['Energy'].to_numpy()\n",
        "# y = y - np.mean(y)\n",
        "X = X[:,np.newaxis] #(N,1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.92, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "1J-aEtmOFMtU",
        "outputId": "1c30d265-7859-4061-c492-33266de2cc9a"
      },
      "outputs": [],
      "source": [
        "# GP\n",
        "x_grid = np.linspace(0.3,2.5,250)[:,np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(X, y,ls='--',c='k',lw=3.,label = 'DFT')\n",
        "plt.scatter(X_train,y_train,marker='o',\\\n",
        "            c='tab:blue',s=100,label='Data',zorder=3.5)\n",
        "\n",
        "l = np.array([0.25, 1., 5.])\n",
        "for li in l:\n",
        "    kernel = 0.01 * \\\n",
        "        RBF(length_scale=li, length_scale_bounds=(1E-3, 1E2))\n",
        "    model = GaussianProcessRegressor(kernel=kernel, optimizer=None)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(x_grid)\n",
        "    plt.plot(x_grid, y_pred, lw=3.,\\\n",
        "        label=r'$\\ell = %.1f$' % li)\n",
        "\n",
        "plt.ylim(np.min(y)*1.01, np.max(y)*0.975)\n",
        "plt.legend(fontsize=17)\n",
        "plt.title('N2 (PES), %s'%model.kernel_, fontsize=20)\n",
        "plt.xlabel(r'R [$\\AA$]', fontsize=20)\n",
        "plt.ylabel('Energy', fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yWiVMTaFXwc"
      },
      "source": [
        "From the figure above we can observe that for the RBF kernel, the length-scale parameter $\\ell$ is fundamental in the accuracy.\n",
        "\n",
        "$$ k_{\\text{RBF}}(x_i, x_j) = \\exp \\left( -\\frac{(x_i - x_j)^2}{2\\ell^2} \\right) $$\n",
        "\n",
        "$\\ell$ dictates how quickly the similarity measure decays with distance. The larger $\\ell$ is, the smoother the function will be. Conversely, a small $\\ell$ leads to a function that reacts more sensitively to the training data, which can capture more detail but might also lead to overfitting.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukOwouBhIynL"
      },
      "source": [
        "### **Marginal Likelihood**\n",
        "\n",
        "The **marginal likelihood** is given by the equation:\n",
        "\n",
        "$$\n",
        "\\log p(\\mathbf{y}|\\mathbf{X}) = -\\frac{1}{2} \\mathbf{y}^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y} - \\frac{1}{2} \\log |K + \\sigma_n^2 I| - \\frac{N}{2} \\log(2\\pi)\n",
        "$$\n",
        "\n",
        "- It represents the probability of generating the observed sample from a prior and is therefore often referred to as **model evidence** or simply evidence.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Kernel Function**\n",
        "\n",
        "Let's consider the following kernel:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "k_{\\text{RBF}}(x_i, x_j) &= C \\exp \\left( -\\frac{(x_i - x_j)^2}{2\\ell^2} \\right)\\\\\n",
        "k_{\\text{Matern}}(d) &= \\left(1 + \\frac{\\sqrt{5}d}{\\ell} + \\frac{5d^2}{3\\ell^2}\\right) \\exp\\left(-\\frac{\\sqrt{5}d}{\\ell}\\right), \\text{  where  } d(x_i, x_j) = (x_i - x_j)^2 \\\\\n",
        "k_{\\text{RQ}}(d) &= \\left(1 + \\frac{d^2}{2\\alpha \\ell^2}\\right)^{-\\alpha}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "> By changing the values of the constant $(C)$ the kernel can be scaled. In Gaussian Process Regression, the log marginal likelihood is used as a measure of model fit; higher values indicate a better-fitting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "lS4e3jCXFXJZ",
        "outputId": "ea4a25e7-1ce6-4333-d156-1d59fb6b8881"
      },
      "outputs": [],
      "source": [
        "def get_mll_curve(c):\n",
        "    l_ = np.linspace(0.01,3.,100) # grid over lenght-scale parameter\n",
        "    mll_ = []\n",
        "\n",
        "    for li in l_:\n",
        "        kernel = c * \\\n",
        "            RBF(length_scale=li, length_scale_bounds=(1E-3, 1E2))\n",
        "        model = GaussianProcessRegressor(kernel=kernel, optimizer=None)\n",
        "        model.fit(X_train, y_train)\n",
        "        mll = model.log_marginal_likelihood()\n",
        "        mll_.append(mll)\n",
        "    return np.array(mll_), l_\n",
        "\n",
        "# grid over the c-parameter\n",
        "c = np.array([0.0005,0.001,0.002,0.003,0.004,0.005])\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "for ci in c:\n",
        "    mll_, l_ = get_mll_curve(ci)\n",
        "    plt.plot(l_,-mll_,label=r'$c = %.4f$'%ci,lw=3.)\n",
        "\n",
        "plt.title('%s' % model.kernel_, fontsize=20)\n",
        "plt.xlabel(r'$\\ell$', fontsize=20)\n",
        "plt.ylabel('neg log marginal likelihood', fontsize=20)\n",
        "plt.xticks(fontsize=15)\n",
        "plt.legend(fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP9H33O2J2Tx"
      },
      "source": [
        "> By observing the peak of each curve, we can deduce the optimal length-scale parameter for each value of **$C$**. This is part of the **hyperparameter tuning** process to find the best settings for the Gaussian Process model to fit the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RBF kernel  ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimal value of the parameters\n",
        "\n",
        "kernel = ConstantKernel(1.) * \\\n",
        "    RBF(length_scale=1., length_scale_bounds=(1E-3, 1E3))\n",
        "gpr_model = GaussianProcessRegressor(\n",
        "    kernel=kernel, n_restarts_optimizer=200, alpha=1E-5)\n",
        "gpr_model.fit(X_train, y_train)\n",
        "print(f\"Kernel parameters before fit:\\n{kernel})\")\n",
        "print(\n",
        "    f\"Kernel parameters after fit: \\n{gpr_model.kernel_} \\n\"\n",
        "    f\"Log-likelihood: {gpr_model.log_marginal_likelihood(gpr_model.kernel_.theta):.3f}\"\n",
        ")\n",
        "\n",
        "k_fit_dict = gpr_model.kernel_.get_params()\n",
        "c0 = k_fit_dict['k1__constant_value']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = 3\n",
        "y_mean, y_std = gpr_model.predict(x_grid, return_std=True)\n",
        "y_samples = gpr_model.sample_y(x_grid, n_samples)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, figsize=(12, 7))\n",
        "for idx, single_prior in enumerate(y_samples.T):\n",
        "    ax.plot(x_grid, single_prior, linestyle=\"--\", alpha=0.7,\n",
        "        label=f\"Sampled function #{idx + 1}\",\n",
        "    )\n",
        "ax.plot(x_grid, y_mean, color=\"black\", label=\"Mean\")\n",
        "ax.fill_between(x_grid.ravel(), y_mean - y_std, y_mean + y_std,\n",
        "                alpha=0.1, color=\"black\", label=r\"$\\pm$ 1 std. dev.\", )\n",
        "ax.scatter(X_train, y_train, color=\"k\",marker='s',s=50)\n",
        "ax.plot(X,y,label='DFT',linestyle=\":\",lw=3,color=\"red\")\n",
        "ax.set_title('N2 (PES), %s' % model.kernel_, fontsize=20)\n",
        "ax.set_xlabel(r'R [$\\AA$]', fontsize=20)\n",
        "ax.set_ylabel('Energy', fontsize=20)\n",
        "plt.legend(fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x0 = np.ones_like(x_grid)\n",
        "x0_grid = np.linspace(-5,5,100)\n",
        "kfit_values = gpr_model.kernel_(x0_grid[:,None],np.zeros((1,1)))/c0\n",
        "k_values = kernel(x0_grid[:, None], np.zeros((1, 1)))\n",
        "\n",
        "fig, ax = plt.subplots(ncols=2, figsize=(12, 7))\n",
        "ax[0].plot(x0_grid,k_values,lw=2)\n",
        "ax[0].text(-5,0.9,'Initial kernel',fontsize=20)\n",
        "ax[0].set_xlabel(r'$x_{i} - x_{j}$ [$\\AA$]', fontsize=20)\n",
        "\n",
        "ax[1].plot(x0_grid, kfit_values, lw=2)\n",
        "ax[1].text(-4.75, 0.9,'Fit kernel',fontsize=20)\n",
        "ax[1].set_xlabel(r'$x_{i} - x_{j}$ [$\\AA$]', fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matern kernel ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimal value of the parameters\n",
        "\n",
        "kernel = ConstantKernel(1.) * \\\n",
        "    Matern(nu=2.5,length_scale=1., length_scale_bounds=(1E-3, 1E3))\n",
        "gpr_model = GaussianProcessRegressor(\n",
        "    kernel=kernel, n_restarts_optimizer=200, alpha=1E-5)\n",
        "gpr_model.fit(X_train, y_train)\n",
        "print(f\"Kernel parameters before fit:\\n{kernel})\")\n",
        "print(\n",
        "    f\"Kernel parameters after fit: \\n{gpr_model.kernel_} \\n\"\n",
        "    f\"Log-likelihood: {gpr_model.log_marginal_likelihood(gpr_model.kernel_.theta):.3f}\"\n",
        ")\n",
        "\n",
        "k_fit_dict = gpr_model.kernel_.get_params()\n",
        "c0 = k_fit_dict['k1__constant_value']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = 3\n",
        "y_mean, y_std = gpr_model.predict(x_grid, return_std=True)\n",
        "y_samples = gpr_model.sample_y(x_grid, n_samples)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, figsize=(12, 7))\n",
        "for idx, single_prior in enumerate(y_samples.T):\n",
        "    ax.plot(x_grid, single_prior, linestyle=\"--\", alpha=0.7,\n",
        "            label=f\"Sampled function #{idx + 1}\",\n",
        "            )\n",
        "ax.plot(x_grid, y_mean, color=\"black\", label=\"Mean\")\n",
        "ax.fill_between(x_grid.ravel(), y_mean - y_std, y_mean + y_std,\n",
        "                alpha=0.1, color=\"black\", label=r\"$\\pm$ 1 std. dev.\", )\n",
        "ax.scatter(X_train, y_train, color=\"k\", marker='s', s=50)\n",
        "ax.plot(X, y, label='DFT', linestyle=\":\", lw=3, color=\"red\")\n",
        "ax.set_title('N2 (PES), %s' % gpr_model.kernel_, fontsize=20)\n",
        "ax.set_xlabel(r'R [$\\AA$]', fontsize=20)\n",
        "ax.set_ylabel('Energy', fontsize=20)\n",
        "plt.legend(fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x0 = np.ones_like(x_grid)\n",
        "x0_grid = np.linspace(-5, 5, 100)\n",
        "kfit_values = gpr_model.kernel_(x0_grid[:, None], np.zeros((1, 1)))/c0\n",
        "k_values = kernel(x0_grid[:, None], np.zeros((1, 1)))\n",
        "\n",
        "fig, ax = plt.subplots(ncols=2, figsize=(12, 7))\n",
        "ax[0].plot(x0_grid, k_values, lw=2)\n",
        "ax[0].text(-5, 0.9, 'Initial kernel', fontsize=20)\n",
        "ax[0].set_xlabel(r'$x_{i} - x_{j}$ [$\\AA$]', fontsize=20)\n",
        "\n",
        "ax[1].plot(x0_grid, kfit_values, lw=2)\n",
        "ax[1].text(-4.75, 0.9, 'Fit kernel', fontsize=20)\n",
        "ax[1].set_xlabel(r'$x_{i} - x_{j}$ [$\\AA$]', fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rational Quadratic kernel ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimal value of the parameters\n",
        "\n",
        "kernel = ConstantKernel(1.) * \\\n",
        "    RationalQuadratic(length_scale=1., length_scale_bounds=(1E-3, 1E3))\n",
        "gpr_model = GaussianProcessRegressor(\n",
        "    kernel=kernel, n_restarts_optimizer=200, alpha=1E-5)\n",
        "gpr_model.fit(X_train, y_train)\n",
        "print(f\"Kernel parameters before fit:\\n{kernel})\")\n",
        "print(\n",
        "    f\"Kernel parameters after fit: \\n{gpr_model.kernel_} \\n\"\n",
        "    f\"Log-likelihood: {gpr_model.log_marginal_likelihood(gpr_model.kernel_.theta):.3f}\"\n",
        ")\n",
        "\n",
        "k_fit_dict = gpr_model.kernel_.get_params()\n",
        "c0 = k_fit_dict['k1__constant_value']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = 3\n",
        "y_mean, y_std = gpr_model.predict(x_grid, return_std=True)\n",
        "y_samples = gpr_model.sample_y(x_grid, n_samples)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, figsize=(12, 7))\n",
        "for idx, single_prior in enumerate(y_samples.T):\n",
        "    ax.plot(x_grid, single_prior, linestyle=\"--\", alpha=0.7,\n",
        "            label=f\"Sampled function #{idx + 1}\",\n",
        "            )\n",
        "ax.plot(x_grid, y_mean, color=\"black\", label=\"Mean\")\n",
        "ax.fill_between(x_grid.ravel(), y_mean - y_std, y_mean + y_std,\n",
        "                alpha=0.1, color=\"black\", label=r\"$\\pm$ 1 std. dev.\", )\n",
        "ax.scatter(X_train, y_train, color=\"k\", marker='s', s=50)\n",
        "ax.plot(X, y, label='DFT', linestyle=\":\", lw=3, color=\"red\")\n",
        "ax.set_title('N2 (PES), %s' % gpr_model.kernel_, fontsize=20)\n",
        "ax.set_xlabel(r'R [$\\AA$]', fontsize=20)\n",
        "ax.set_ylabel('Energy', fontsize=20)\n",
        "plt.legend(fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x0 = np.ones_like(x_grid)\n",
        "x0_grid = np.linspace(-5, 5, 100)\n",
        "kfit_values = gpr_model.kernel_(x0_grid[:, None], np.zeros((1, 1)))/c0\n",
        "k_values = kernel(x0_grid[:, None], np.zeros((1, 1)))\n",
        "\n",
        "fig, ax = plt.subplots(ncols=2, figsize=(12, 7))\n",
        "ax[0].plot(x0_grid, k_values, lw=2)\n",
        "ax[0].text(-5, 0.9, 'Initial kernel', fontsize=20)\n",
        "ax[0].set_xlabel(r'$x_{i} - x_{j}$ [$\\AA$]', fontsize=20)\n",
        "\n",
        "ax[1].plot(x0_grid, kfit_values, lw=2)\n",
        "ax[1].set_xlabel(r'$x_{i} - x_{j}$ [$\\AA$]', fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUGAHDjNKrx7"
      },
      "source": [
        "### **Homework - Fitting a Gaussian Process on an ${H}_2O$ Potential Energy Surface (PES)**\n",
        "\n",
        ">## **Resources**\n",
        ">\n",
        ">- **PyScf**: [Official Documentation](https://pyscf.org/user.html)\n",
        "- **Assessing Gaussian Process Regression To Represent High-Dimensional Potential Energy Surfaces**: [Research Paper](https://pubs.acs.org/doi/10.1021/acs.jctc.8b00298)\n",
        "- **A tutorial on Gaussian process regression: Modelling, exploring, and exploiting unknown functions**: [Journal Article](https://www.biorxiv.org/content/10.1101/095190v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHShmQEpLJ53"
      },
      "outputs": [],
      "source": [
        "# Your Code Here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
