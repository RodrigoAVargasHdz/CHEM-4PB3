{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open on Google Colab [link](https://colab.research.google.com/github/RodrigoAVargasHdz/CHEM-4PB3/blob/main/Course_Notes/Week9/property_prediction.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cairosvg\n",
    "!pip install rdkit-pypi\n",
    "!pip install avogadro\n",
    "!pip install py3Dmol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw, rdMolDescriptors, rdDistGeom, rdMolTransforms, QED\n",
    "from rdkit.Chem.Scaffolds.MurckoScaffold import GetScaffoldForMol\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "data_url = \"https://github.com/RodrigoAVargasHdz/CHEM-4PB3/raw/main/Course_Notes/data/qm9.csv\"\n",
    "data_big = pd.read_csv(data_url)\n",
    "data_big.head()\n",
    "n0 = 30000\n",
    "data = data_big.sample(n0)\n",
    "print(data.shape)\n",
    "print('Properties', data.columns)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_all = data['smiles']\n",
    "smiles_len = list(map(len, smiles_all))\n",
    "max_len = max(smiles_len) + 5\n",
    "\n",
    "SMILES_CHARS = [\"7\", \"6\", \"o\", \"]\", \"3\", \"s\", \"(\", \"-\", \"S\", \"/\", \"B\", \"4\", \"[\", \")\", \"#\", \"I\",\n",
    "                \"l\", \"O\", \"H\", \"c\", \"1\", \"@\", \"=\", \"n\", \"P\", \"8\", \"C\", \"2\", \"F\", \"5\", \"r\", \"N\", \"+\", \"\\\\\", \" \"]\n",
    "# index\n",
    "smi2index = dict((c, i) for i, c in enumerate(SMILES_CHARS))\n",
    "\n",
    "def smiles_to_one_hot(smiles, maxlen=max_len):\n",
    "    X = np.zeros((maxlen, len(SMILES_CHARS)))  # (maxlen, dictionary)\n",
    "    # print(smiles,type(smiles))\n",
    "    smiles = smiles.replace('\\n', '')\n",
    "    for i, c in enumerate(smiles):\n",
    "        X[i, smi2index[c]] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_MorganFP(smiles,radius=2):\n",
    "    mol = AllChem.MolFromSmiles(smiles)\n",
    "\n",
    "    bi = {}\n",
    "    fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(\n",
    "        mol, radius=radius, bitInfo=bi)\n",
    "    return np.array(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_classicalMD(smiles):\n",
    "    m = AllChem.MolFromSmiles(smiles)\n",
    "    # canon_smiles = AllChem.MolToSmiles(m, canonical=True)\n",
    "    # number of H-bond acceptors for a molecule\n",
    "    hba = rdMolDescriptors.CalcNumHBA(m)\n",
    "\n",
    "    # number of H-bond donors for a molecule\n",
    "    hbd = rdMolDescriptors.CalcNumHBD(m)\n",
    "\n",
    "    # number of rings for a molecule\n",
    "    nrings = rdMolDescriptors.CalcNumRings(m)\n",
    "\n",
    "    # number of rotatable bonds for a molecule\n",
    "    rtb = rdMolDescriptors.CalcNumRotatableBonds(m)\n",
    "\n",
    "    #  topological polar surface area (TPSA) of a molecule (used medicinal chemistry metric for the optimization of a drug's ability to permeate cells.)\n",
    "    psa = rdMolDescriptors.CalcTPSA(m)\n",
    "\n",
    "    # logP and mr from https://pubs.acs.org/doi/10.1021/ci990307l:\n",
    "    # logP ->  water partition coefficient as measure of lipophilicity\n",
    "    # MR -> molar refractivity\n",
    "    logp, mr = rdMolDescriptors.CalcCrippenDescriptors(m)\n",
    "\n",
    "    # molecular weight\n",
    "    mw = rdMolDescriptors._CalcMolWt(m)\n",
    "\n",
    "    # Csp3: fraction of sp3 carbons\n",
    "    csp3 = rdMolDescriptors.CalcFractionCSP3(m)\n",
    "\n",
    "    # fraction of atoms belonging to Murcko framework\n",
    "    # number of heavy atoms for a molecule\n",
    "    fmf = GetScaffoldForMol(m).GetNumHeavyAtoms() / m.GetNumHeavyAtoms()\n",
    "    hac = m.GetNumHeavyAtoms()\n",
    "\n",
    "    # max_ring_size: maximum ring size in a molecule\n",
    "    max_ring_size = len(max(m.GetRingInfo().AtomRings(), key=len, default=()))\n",
    "\n",
    "    # QED: quantitative estimate of drug-likeness (https://www.rdkit.org/docs/source/rdkit.Chem.QED.html)\n",
    "    qed = QED.qed(m)\n",
    "\n",
    "    # ChiralCenters: number of chiral centers (assigned and unassigned)\n",
    "    n_chiral_centers = len(\n",
    "        Chem.FindMolChiralCenters(m, includeUnassigned=True))\n",
    "\n",
    "    # r = [hba, hbd, nrings, rtb, logp, mr, mw, csp3, fmf, qed, hac, n_chiral_centers, max_ring_size]\n",
    "    r = [hba, hbd, nrings, rtb, mr, mw, csp3, fmf, qed, hac, n_chiral_centers, max_ring_size]\n",
    "    return np.array(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data class\n",
    "class MolecDataset(Dataset):\n",
    "    def __init__(self, smiles_all, molecules_labels_all, fp_style='one_hot', bool_flatten = False):\n",
    "        self.labels_all = molecules_labels_all\n",
    "        self.smiles_all = smiles_all\n",
    "        self.fp_style = fp_style\n",
    "        self.bool_flatten = bool_flatten\n",
    "        self.max_len = 35\n",
    "        self.radius = 3\n",
    "        # self.flatten = flatten\n",
    "        # self.one_hot_label = one_hot\n",
    "        # self.cnn = cnn\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_all)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_all[idx]\n",
    "        # smiles_canon = AllChem.MolToSmiles(AllChem.MolFromSmiles(smiles), canonical=True)\n",
    "        label = self.labels_all[idx]\n",
    "        if self.fp_style == 'one_hot':\n",
    "            \"\"\"\n",
    "            creates a one-hot encoding representation of a smiles.\n",
    "            Maxlength is 100\n",
    "\n",
    "            Returns:\n",
    "                z: one-hot molecule representation (35,100)\n",
    "            \"\"\"\n",
    "            z = torch.tensor(smiles_to_one_hot(smiles, self.max_len)).double()\n",
    "            z = z.unsqueeze(0).float() # add one channel \n",
    "        elif self.fp_style == 'ecf':\n",
    "            \"\"\"generates a Finger Print representation using the Morgan algorithm with radius = 2\n",
    "            \"\"\"\n",
    "            z = smiles_to_MorganFP(smiles, self.radius)\n",
    "            z = torch.tensor(z).double()\n",
    "            z = z.unsqueeze(0).float()\n",
    "        elif self.fp_style == 'md':\n",
    "            \"\"\"generates a Finger Print representation using the some Molecular Descriptors\n",
    "            \"\"\"\n",
    "            z = smiles_to_classicalMD(smiles)\n",
    "            z = torch.tensor(z).double()\n",
    "            z = z.unsqueeze(0).float()\n",
    "            \n",
    "        if self.bool_flatten:\n",
    "            z = torch.flatten(z,start_dim=1)\n",
    "        \n",
    "        molecule = z\n",
    "    \n",
    "        return molecule, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = smiles_all.to_list()\n",
    "properties = ['homo', 'lumo']  # data[['smiles','homo']]\n",
    "y_all = data[properties].to_numpy()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# load the data\n",
    "\n",
    "Xtr, Xtest, ytr, ytest = train_test_split(\n",
    "    X_all, y_all, test_size=0.75, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "\n",
    "  def __init__(self,x_dim,z_dims):\n",
    "    super().__init__()\n",
    "    self.encoder_layers = nn.Sequential(\n",
    "        # nn.Flatten(),\n",
    "        nn.Linear(x_dim, z_dims[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(z_dims[0], z_dims[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(z_dims[1], z_dims[2]),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    self.final_layer = nn.Linear(z_dims[2], z_dims[3])\n",
    "    \n",
    "  def encoder(self,x):\n",
    "    return self.encoder_layers(x.flatten(start_dim=1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    z = self.encoder(x.flatten(start_dim=1))\n",
    "    return self.final_layer(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,n_outputs):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, 3, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(16 * 9 * 9, n_outputs)\n",
    "    \n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.conv2,\n",
    "            self.conv3,\n",
    "        )\n",
    "    def encoder(self,x):\n",
    "        return self.encoder_layers(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecdata_tr = MolecDataset(Xtr, torch.tensor(ytr), 'one_hot')\n",
    "\n",
    "train_dataloader = DataLoader(molecdata_tr, batch_size=10, shuffle=True)\n",
    "for d_epoch in train_dataloader:\n",
    "    x,y = d_epoch\n",
    "    break\n",
    "print('Batch size')\n",
    "print(x.shape,y.shape)\n",
    "print('===================')\n",
    "\n",
    "def get_x_dim(x):\n",
    "    dim = 1\n",
    "    for i in x:\n",
    "       dim = dim*i \n",
    "    return dim\n",
    "        \n",
    "n_outputs = 2\n",
    "x_dim = get_x_dim(x.shape[1:])\n",
    "print(x.shape[1:],x_dim)\n",
    "\n",
    "print('===================')\n",
    "print('MLP')\n",
    "mlp_model = MLP(x_dim,[3000,3000,3000,2])\n",
    "# print(mlp_model(x).shape)\n",
    "# print(mlp_model.encoder(x).shape)\n",
    "# print('Network parameters')\n",
    "for n,p in mlp_model.named_parameters():\n",
    "    print(n,p.shape)\n",
    "\n",
    "print('===================')\n",
    "print('CNN')\n",
    "cnn_model = CNN(2)\n",
    "print(cnn_model.encoder(x).shape)\n",
    "print(cnn_model(x).shape)\n",
    "for n,p in cnn_model.named_parameters():\n",
    "    print(n,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_data, training_epochs=60):\n",
    "    # Define the loss function and optimizer\n",
    "    \n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.02)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        training_data, batch_size=128, shuffle=True)\n",
    "\n",
    "    iterator = tqdm.notebook.tqdm(range(training_epochs))\n",
    "\n",
    "    # Run the training loop (epochs)\n",
    "    loss_trajectory = []\n",
    "    for epoch in iterator:\n",
    "\n",
    "        # Set current loss value\n",
    "        current_loss = []\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, targets = data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_function(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            # current_loss += loss.item()\n",
    "            current_loss.append(loss.item())\n",
    "        # print('Epoch %s: %.4f +- %.4f'%(epoch,np.array(current_loss).mean(),np.array(current_loss).std()))\n",
    "        iterator.set_postfix(loss=torch.tensor(current_loss).mean())\n",
    "        loss_trajectory.append(current_loss)\n",
    "        # Process is complete.\n",
    "    return loss_trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model\n",
    "loss_trajectory = train(cnn_model,molecdata_tr,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(np.arange(len(loss_trajectory[0])),np.array(loss_trajectory[0]))\n",
    "plt.xlabel('Iterations',fontsize = 15)\n",
    "plt.ylabel('Loss function', fontsize=15)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is our model? \n",
    "Using the remaining data set make a ```pred vs exact``` plot and compute the RMSE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecdata_tst = MolecDataset(Xtest, torch.tensor(ytest), 'one_hot')\n",
    "\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        molecdata_tst, batch_size=128, shuffle=False)\n",
    "    \n",
    "    outputs_pred = torch.zeros((1,2)) # number of outputs\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "            inputs, targets = data\n",
    "\n",
    "            outputs = cnn_model(inputs)\n",
    "            outputs_pred = torch.vstack((outputs_pred,outputs))\n",
    "\n",
    "outputs_pred = outputs_pred[1:]\n",
    "print(torch.tensor(ytest).shape)\n",
    "print(outputs_pred.shape)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "print(loss_function(outputs_pred,torch.tensor(ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "(from [Wiki](https://en.wikipedia.org/wiki/Transfer_learning))\\\n",
    "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.\\\n",
    "For example, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks.\\\n",
    "From the practical standpoint, reusing or transferring information from previously learned tasks for the learning of new tasks has the potential to significantly improve the sample efficiency of a reinforcement learning agent.\n",
    "\n",
    "**How can we use transfer learning for Chemistry?**\n",
    "\n",
    "<img src=\"https://raw.github.com/cs231n/cs231n.github.io/master/assets/nn1/neural_net2.jpeg\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_net_parameters(net):\n",
    "    for name, para in net.named_parameters():\n",
    "        print(\"-\"*20)\n",
    "        print(f\"name: {name}\")\n",
    "        print(\"size: \", para.shape)\n",
    "        print(\"Grad: \", para.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_net_parameters(cnn_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization in ML is based on gradients\n",
    "\n",
    "If we do not update part of the parameters in the training we can use them as a \"fixed\" encoding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in cnn_model.named_parameters():\n",
    "     if param.requires_grad and 'conv' in name:\n",
    "         param.requires_grad = False\n",
    "print_net_parameters(cnn_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class exercise  \n",
    "Re-train a model to predict the ```logP``` without touching the \"encoder\" to predict the logP. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = 10000\n",
    "data = data_big.sample(n0)\n",
    "smiles_logP_all = data['smiles'].to_list()\n",
    "\n",
    "logP_all = []\n",
    "for s in smiles_logP_all:\n",
    "    m = AllChem.MolFromSmiles(s)\n",
    "    logp, mr = rdMolDescriptors.CalcCrippenDescriptors(m)\n",
    "    logP_all.append(logp)\n",
    "\n",
    "logP_all = np.array(logP_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_all = smiles_logP_all\n",
    "y_logP_all = logP_all\n",
    "\n",
    "# load the data\n",
    "\n",
    "Xtr, Xtest, ytr, ytest = train_test_split(\n",
    "    X_log_all, y_logP_all, test_size=0.5, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logPdata_tr = MolecDataset(Xtr, torch.tensor(ytr), 'one_hot')\n",
    "\n",
    "cnn_model_logP = CNN(1)\n",
    "print(cnn_model_logP.conv1.\n",
    "# for (name, param),(name1, param1) in zip(cnn_model.named_parameters(),cnn_model_logP.named_parameters()):\n",
    "    # if 'conv1' in name and 'conv1' in name1:\n",
    "        # print(name)\n",
    "    #    cnn_model_logP.name.weight.data = cnn_model.name.weight.data\n",
    "        # param1 = param\n",
    "    # print(name,name1)\n",
    "    # print(param.shape,param1.shape)\n",
    "# loss_trajectory_logP = train(cnn_model,logPdata_tr,2)\n",
    "# print()\n",
    "\n",
    "for (name, param),(name1, param1) in zip(cnn_model.named_parameters(),cnn_model_logP.named_parameters()):\n",
    "    print(param-param1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        molecdata_tst, batch_size=2, shuffle=False)\n",
    "    \n",
    "    outputs_pred = torch.zeros((1,2)) # number of outputs\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "            inputs, targets = data\n",
    "            print(cnn_model(input))\n",
    "            \n",
    "            # outputs = cnn_model(inputs)\n",
    "            # outputs_pred = torch.vstack((outputs_pred,outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem4pb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
