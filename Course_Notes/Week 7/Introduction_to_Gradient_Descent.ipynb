{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tVDZ3OLoxr_APkVcYrXn8fJxFDr3_k2k?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoJAA55DY5l8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1poOlDxSaRf"
      },
      "source": [
        "# **Week 7 - Introduction to Gradient Descent**\n",
        "\n",
        "## **Abstract**\n",
        "\n",
        "1. **Understanding Gradient Descent**: Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. It's widely used in machine learning for efficiently finding the optimal parameters of models.\n",
        "\n",
        "2. **Introduction to the Jacobian**: The Jacobian matrix is a key concept in multivariable calculus, representing all first-order partial derivatives of a vector-valued function. It provides crucial insights into how changes in input variables affect the output, making it vital for understanding the dynamics of complex systems.\n",
        "\n",
        "\n",
        "3. **Fitting the Gradient Descent to N-N**: Applying Gradient Descent with a Harmonic Series to fit a Potential Energy Surface (PES) to N-N using PyTorch.\n",
        "\n",
        "\n",
        "\n",
        ">### **References: Essential Resources for Further Learning**\n",
        ">\n",
        ">- **PyTorch**: [Official Documentation](https://pytorch.org/docs/stable/index.html)\n",
        ">- **A Gentle Introduction to the Jacobian**: [Online Course](https://machinelearningmastery.com/a-gentle-introduction-to-the-jacobian/)\n",
        ">- **A tutorial on Gaussian process regression: Modelling, exploring, and exploiting unknown functions**: [Journal Article](https://www.biorxiv.org/content/10.1101/095190v3)\n",
        ">- **Gradient fit functions for two-body potential energy surfaces based upon a harmonic series**: [Research Paper](https://www.tandfonline.com/doi/full/10.1080/08927020903433739)\n",
        "\n",
        "\n",
        "\n",
        "Feel free to explore these resources to deepen your understanding of data visualization, data management, and computational tools in Chemistry.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNmRFyA77H1_"
      },
      "source": [
        "# Linear Model Overview #\n",
        "\n",
        "The linear model, a fundamental concept in machine learning, can be dissected as follows:\n",
        "\n",
        "- **Linear Model Representation**:\n",
        "  - The model is defined as $f_{\\phi(\\cdot)}(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) = \\sum_i w_i \\phi_i(\\mathbf{x})$.\n",
        "  - Here, $\\phi(\\cdot)$ represents a linear map to a new **feature representation** (e.g., polynomials, Fourier basis).\n",
        "\n",
        "- **Square Loss Function**:\n",
        "  - Denoted by ${L} \\approx \\|\\mathbf{y} - f_{\\phi(\\cdot)}(\\mathbf{X}) \\|_2$.\n",
        "  - It measures the Euclidean distance between the model's predictions and the actual target values.\n",
        "\n",
        "- **Optimal Parameters ($\\mathbf{w}^*$)**:\n",
        "  - Obtained in **closed form** as $\\mathbf{w}^* = \\Big( \\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X})\\Big)^{-1} \\Phi(\\mathbf{X})^\\top\\mathbf{y}$.\n",
        "  - This solution requires **matrix inversion**, an operation with computational complexity ${O}(N^3)$.\n",
        "\n",
        "- **Computational Challenge**:\n",
        "  - Inverting the matrix $\\Big( \\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X})\\Big)$ is the bottleneck due to its high computational cost.\n",
        "  - With a large dataset (e.g., $N \\approx 100,000$), this becomes significantly demanding.\n",
        "\n",
        "- **Gradient of Loss Function**:\n",
        "  - Expressed as $\\nabla {L}(\\mathbf{w}) = \\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X})\\mathbf{w} -\\Phi(\\mathbf{X})^\\top\\mathbf{y}$.\n",
        "  - Notably, the computational complexity here scales **quadratically** (${O}(N^2)$), which is more efficient than matrix inversion.\n",
        "\n",
        "- **Scaling Perspective**:\n",
        "  - For a dataset with $N = 100$, the quadratic and cubic scalings result in $N^2 = 10,000$ and $N^3 = 1,000,000$, respectively.\n",
        "\n",
        "The above breakdown showcases the linear model's structure and highlights the computational challenges, especially with large datasets. Understanding these aspects is crucial for selecting appropriate models and optimization strategies in machine learning.\n",
        "\n",
        "<br>\n",
        "\n",
        "> A closed-form solution is an exact solution expressed as a straightforward expression. For example, in the case of linear regression, the optimal weights $\\mathbf{w}^*$ can be found directly using the formula:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqsMjUGv9fmK"
      },
      "source": [
        "## The Kernel Trick ##\n",
        "\n",
        "- **Dimensionality Transformation for Extrapolation**: The transformation of input space to a feature space via a **kernel function $\\phi$** is especially beneficial in extrapolation tasks.\n",
        "  - **Enhanced Pattern Recognition**: In the feature space, the model can recognize complex patterns and trends that are not apparent in the original input space. This improved pattern recognition is crucial for making predictions beyond the range of observed data.\n",
        "  - **Linear Separability in Feature Space**: While the original input space might display non-linear relationships, the feature space is often linearly separable. This property allows the model to extrapolate more effectively, as linear relationships are easier to extend beyond the known data.\n",
        "  - **Implicit High-Dimensional Mapping**: The kernel trick enables this transformation without the computational expense of directly calculating high-dimensional representations, which is instrumental for the model to make accurate extrapolations.\n",
        "\n",
        "<br>\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td style=\"text-align: center;\">\n",
        "      <img src=\"https://images.squarespace-cdn.com/content/v1/51af568be4b0b9ab836e2474/1444337178607-L2CH88QNWQ7M7KIB907P/2d_boundary.png?format=1000w\" alt=\"2D Boundary\" height=\"400\"/>\n",
        "      <br>2D view of boundary\n",
        "    </td>\n",
        "    <td style=\"text-align: center;\">\n",
        "      <img src=\"https://images.squarespace-cdn.com/content/v1/51af568be4b0b9ab836e2474/1444337191389-5TTD0VOCK8R2QSHTRGJ2/boundary.gif\" alt=\"Boundary GIF\" height=\"400\"/>\n",
        "      <br>3D view of boundary plane\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "[**Figure 1 - The \"Kernel Trick\" Visualized. Image by Ilan Man**](https://www.ilanman.io/blog/2015/10/9/not-your-grandfathers-kernel-trick)\n",
        "\n",
        "\n",
        "> **Transformation Function** **$\\phi$**: Represents the kernel trick, an implicit function that transformes the data to a higher dimension. In higher dimensions, the data can be segmented into groups more efficiently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBoIjEzk-M39"
      },
      "source": [
        "# Gradient Descent: An Introduction in the Context of Linear Models #\n",
        "\n",
        "Gradient descent is an optimization technique in machine learning, especially useful when dealing with complex models where closed-form solutions, like those in simpler linear models, are not feasible. As we've seen in the context of linear models, the concept of gradients plays a crucial role in understanding and optimizing the model's performance.\n",
        "\n",
        "### **Understanding Gradient Descent:**\n",
        "\n",
        "- **Basic Principle**: Gradient descent is an iterative method used to find the **minimum or maximum** of a function. <br> \n",
        "  This method is particularly effective for functions where calculating the exact minimum is not straightforward or computationally feasible.\n",
        "\n",
        "- **Application in Linear Models**: Recall from our discussion on linear models, the gradient of the loss function $L$ with respect to the weights $\\mathbf{w}$ was given by \n",
        "    $$ \n",
        "    \\nabla L(\\mathbf{w}) = \\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X})\\mathbf{w} - \\Phi(\\mathbf{X})^\\top\\mathbf{y}\n",
        "    $$\n",
        "    In cases where this cannot be minimized directly through a closed-form solution, gradient descent becomes invaluable.\n",
        "\n",
        "- **Mechanism**: In gradient descent, we start with an initial guess of the parameters (e.g., the weights in a linear model) and iteratively adjust these parameters in the direction that reduces the loss function. <br> The direction is determined by the gradient of the function at that point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "dPbl_Qzk6d7x",
        "outputId": "53a99fe3-d23a-4896-9591-7ba7cd43b605"
      },
      "outputs": [],
      "source": [
        "# Function to compute the sum of squares\n",
        "f_sqr = lambda x: np.sum(np.power(x, 2), axis=1)\n",
        "\n",
        "# Generating the grid\n",
        "x = np.linspace(-2., 2., 100)\n",
        "x0, x1 = np.meshgrid(x, x)\n",
        "X = np.column_stack((x0.flatten(), x1.flatten()))\n",
        "\n",
        "# Compute function values\n",
        "y = f_sqr(X)\n",
        "y = y.reshape((x0.shape))\n",
        "\n",
        "# 3D surface plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "surf = ax.plot_surface(x0, x1, y, cmap='viridis', edgecolor='none')\n",
        "\n",
        "# Adding a color bar\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5, label='f(x0, x1) value')\n",
        "\n",
        "# Setting labels and title\n",
        "ax.set_xlabel('$x_0$', fontsize=12)\n",
        "ax.set_ylabel('$x_1$', fontsize=12)\n",
        "ax.set_zlabel('$f(x_0, x_1)$', fontsize=12)\n",
        "ax.set_title('$f(x_0, x_1) = x_0^2 + x_1^2$', fontsize=15)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsw0VMwO-vR0"
      },
      "source": [
        "### **Gradient Descent in Action:**\n",
        "\n",
        "- **Simple Example**: Consider a function $f(x_0,x_1) = x_0^2 + x_1^2$. This function represents a simple quadratic bowl with its minimum at $(x_0, x_1) = (0, 0)$.\n",
        "\n",
        "- **Gradient Computation**: The gradient of this function is $\\nabla f(x_0, x_1) = [2x_0, 2x_1]$. This gradient points in the direction of the steepest ascent of the function.\n",
        "\n",
        "- **Iterative Process**: Starting from an initial point, say $(x_0, x_1) = (1, 1)$, gradient descent will use the gradient at this point to determine the direction to move in order to minimize $f$. By repeatedly taking steps opposite to the gradient, the algorithm converges to the minimum of the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QiK7pfP--92"
      },
      "source": [
        "<br>\n",
        "\n",
        "<a href=\"https://github.com/rudra-sondhi/Machine-Learning-Animations\" target=\"_blank\">\n",
        "  <img src=\"https://raw.githubusercontent.com/rudra-sondhi/Machine-Learning-Animations/main/Gradient%20Descent/Animations/gradient_descent_simple.gif\" alt=\"Illustrating Gradient Descent\" width=\"750\">\n",
        "</a>\n",
        "<br>\n",
        "</a>\n",
        "\n",
        "\n",
        "[**Figure 2 - Illustrating Gradient Descent.**](https://github.com/rudra-sondhi/Machine-Learning-Animations/blob/main/Gradient%20Descent/Animations/gradient_descent_simple.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCtwJU5x-9RL"
      },
      "source": [
        "### **Advantages and Limitations:**\n",
        "\n",
        "- **Flexibility**: Gradient descent is versatile and can be applied to a wide range of functions, not just simple quadratic ones.\n",
        "\n",
        "- **Computational Efficiency**: For very large datasets or complex models, gradient descent provides a practical way to approximate the optimal solution with reasonable computational resources.\n",
        "\n",
        "- **Sensitivity to Initial Conditions and Learning Rate**: The starting point and the size of the steps (learning rate) can significantly affect the convergence and outcome of the algorithm.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<a href=\"https://github.com/rudra-sondhi/Machine-Learning-Animations\" target=\"_blank\">\n",
        "  <img src=\"https://raw.githubusercontent.com/rudra-sondhi/Machine-Learning-Animations/main/Gradient%20Descent/Animations/gradient_descent_animation.gif\" alt=\"Varied Initial Conditions for Gradient Descent\" width=\"750\">\n",
        "</a>\n",
        "<br>\n",
        "</a>\n",
        "\n",
        "\n",
        "[**Figure 3 - Varied Initial Conditions for Gradient Descent.**](https://github.com/rudra-sondhi/Machine-Learning-Animations/blob/main/Gradient%20Descent/Animations/gradient_descent_animation.gif)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "> Notice the initial conditions and how the learning rate varied till convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI9C237MEJJt"
      },
      "source": [
        "## **Introduction to the Jacobian**\n",
        "\n",
        "In the context of the function $f(x_0, x_1) = x_0^2 + x_1^2$, the Jacobian plays a crucial role in understanding how changes in the input variables ($x_0$ and $x_1$) affect changes in the output of the function. The Jacobian is a matrix that consists of all the first-order partial derivatives of a vector-valued function. For a scalar function like $f(x_0, x_1)$, the Jacobian reduces to the gradient of the function.\n",
        "\n",
        "Here's a brief overview of its application:\n",
        "\n",
        "- **Function Description**: The given function $f(x_0, x_1) = x_0^2 + x_1^2$ is a simple quadratic function, representing a paraboloid in a two-dimensional space.\n",
        "\n",
        "- **Jacobian Calculation**: For this function, the Jacobian (or the gradient, in this case) is a vector containing the partial derivatives with respect to each variable:\n",
        "  \n",
        "  $$\n",
        "  \\nabla f = \\begin{bmatrix}\n",
        "              \\frac{\\partial f}{\\partial x_0} \\\\\n",
        "              \\frac{\\partial f}{\\partial x_1}\n",
        "            \\end{bmatrix}\n",
        "          = \\begin{bmatrix}\n",
        "              2x_0 \\\\\n",
        "              2x_1\n",
        "            \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- **Application**: The Jacobian (gradient) indicates the direction of the steepest ascent in the function's output. In optimization, particularly in gradient descent, this information is used to iteratively move towards the minimum of the function. Since $f(x_0, x_1)$ is convex, following the negative of the gradient will lead to the global minimum.\n",
        "\n",
        "> In simpler terms, the Jacobian can be considered as the **\"first derivative\"** for functions defined by matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIdYTxDGEpUJ"
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <style>\n",
        "        .centered-image {\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 50%;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<a href=\"https://github.com/rudra-sondhi/Machine-Learning-Animations\" target=\"_blank\">\n",
        "    <img src=\"https://raw.githubusercontent.com/rudra-sondhi/Machine-Learning-Animations/main/Gradient%20Descent/Animations/gradient_descent_side_by_side_animation.gif\"\n",
        "         alt=\"Varied Initial Conditions for Gradient Descent\"\n",
        "         class=\"centered-image\">\n",
        "</a>\n",
        "\n",
        "<br>\n",
        "<figcaption align = \"center\"><b>Figure 4 - Varied Initial Conditions for Gradient Descent.</b></figcaption>\n",
        "\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS6Wzi_HGght"
      },
      "source": [
        "## **Gradient Descent for Minimum Search**\n",
        "\n",
        "Gradient descent is a fundamental algorithm for finding the minimum of a function, such as $f(x_0, x_1) = x_0^2 + x_1^2$. Here's a concise overview of the process:\n",
        "\n",
        "1. **Initialization**: Begin with random values of $x_0$ and $x_1$, denoted as $[x_0,x_1]_{t}$.\n",
        "\n",
        "2. **Gradient Computation**: Calculate the gradient, $\\nabla f(x_0,x_1) = \\nabla f([x_0,x_1]_{t})$, which points in the direction of the steepest ascent.\n",
        "\n",
        "3. **Update Step**:\n",
        "   - Adjust $x_0$ and $x_1$ using the gradient: $[x_0,x_1]_{t+1} = [x_0,x_1]_t - \\eta \\nabla f(x_0,x_1)$.\n",
        "   - The parameter $\\eta$ is the **learning rate**, controlling the size of each update.\n",
        "\n",
        "4. **Convergence Check**:\n",
        "   - Continue steps 2 and 3 until the gradient is sufficiently small ($\\|\\nabla f(x_0,x_1) \\| < 10^{-6}$) or the change in function value between iterations is below a threshold ($\\| f([x_0,x_1]_{t+1}) - f([x_0,x_1]_{t})\\| < 10^{-6}$).\n",
        "   - The value $10^{-6}$ is an example threshold and can be adjusted based on the specific problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEqtcZJtHrNi",
        "outputId": "7572d6b5-66f8-4a42-b1c4-7813df94a3df"
      },
      "outputs": [],
      "source": [
        "# Define the gradient and function for gradient descent\n",
        "def grad_f_sqr(x):\n",
        "    # Placeholder gradient function\n",
        "    return 2 * x\n",
        "\n",
        "f_sqr = lambda x: np.sum(np.power(x, 2), axis=1)\n",
        "\n",
        "# Gradient Descent Algorithm\n",
        "\n",
        "# Initializing variables\n",
        "x = np.random.uniform(-2., 2., (1, 2))\n",
        "eta = 0.1\n",
        "\n",
        "# Preparing trajectory storage\n",
        "x_trajectory = np.zeros_like(x.ravel())\n",
        "g_trajectory = np.zeros_like(x.ravel())\n",
        "f_trajectory = np.zeros(1)\n",
        "c = 1E-4\n",
        "xt = x\n",
        "\n",
        "# Gradient Descent Iteration\n",
        "for t in range(20):\n",
        "    grad_t = grad_f_sqr(xt)\n",
        "    y_t = np.sum(xt ** 2)\n",
        "    g_trajectory = np.vstack((g_trajectory, grad_t))\n",
        "    f_trajectory = np.append(f_trajectory, y_t)\n",
        "    x_trajectory = np.vstack((x_trajectory, xt))\n",
        "\n",
        "    # Improved print formatting using f-string\n",
        "    print(f't = {t}, f = {y_t:.4f}, x = {xt}, g = {grad_t}')\n",
        "\n",
        "    xt = xt - eta * grad_t\n",
        "\n",
        "    # Break condition based on gradient and function value change\n",
        "    if np.linalg.norm(g_trajectory[-2] - g_trajectory[-1]) < c or np.linalg.norm(f_trajectory[-2] - f_trajectory[-1]) < c:\n",
        "        break\n",
        "\n",
        "# Data Preparation for Plotting\n",
        "# Remove initial placeholder points\n",
        "g_trajectory = g_trajectory[1:]\n",
        "f_trajectory = f_trajectory[1:]\n",
        "x_trajectory = x_trajectory[1:]\n",
        "\n",
        "# Using predefined values for the contour plot\n",
        "x = np.linspace(-2., 2., 50)\n",
        "x0, x1 = np.meshgrid(x, x)\n",
        "X = np.column_stack((x0.flatten(), x1.flatten()))\n",
        "y = f_sqr(X)\n",
        "y = y.reshape((x0.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mex_hat(x):\n",
        "    r = np.sum(x**2,axis=1,keepdims=True)\n",
        "    print(r.shape)\n",
        "    return -5*r + r**2\n",
        "\n",
        "\n",
        "x = np.linspace(-3., 3., 50)\n",
        "x0, x1 = np.meshgrid(x, x)\n",
        "X = np.column_stack((x0.flatten(), x1.flatten()))\n",
        "y = mex_hat(X)\n",
        "y = y.reshape((x0.shape))\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "# Contour plot with colormap and colorbar\n",
        "\n",
        "contour = ax.contourf(x0, x1, y, levels=60, cmap='viridis')\n",
        "plt.colorbar(contour, ax=ax, label='Function Value')\n",
        "\n",
        "# Trajectory plot\n",
        "ax.plot(x_trajectory[:, 0], x_trajectory[:, 1], color='red',\n",
        "        marker='o', linestyle='-', label=r'$[x_0, x_1]_t$')\n",
        "\n",
        "# Labels and Title\n",
        "plt.figure(figsize=(10, 8))\n",
        "ax.set_xlabel(r'$x_0$', fontsize=15)\n",
        "ax.set_ylabel(r'$x_1$', fontsize=15)\n",
        "ax.set_title('Gradient Descent Trajectory', fontsize=18)\n",
        "\n",
        "# Legend\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n",
        "assert 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "F-IqX7v2IViI",
        "outputId": "ff4e073a-81b1-4312-a0b3-af7fdced69de"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "# Contour plot with colormap and colorbar\n",
        "\n",
        "contour = ax.contourf(x0, x1, y, levels=60, cmap='viridis')\n",
        "plt.colorbar(contour, ax=ax, label='Function Value')\n",
        "\n",
        "# Trajectory plot\n",
        "ax.plot(x_trajectory[:, 0], x_trajectory[:, 1], color='red', marker='o', linestyle='-', label=r'$[x_0, x_1]_t$')\n",
        "\n",
        "# Labels and Title\n",
        "plt.figure(figsize=(10, 8))\n",
        "ax.set_xlabel(r'$x_0$', fontsize=15)\n",
        "ax.set_ylabel(r'$x_1$', fontsize=15)\n",
        "ax.set_title('Gradient Descent Trajectory', fontsize=18)\n",
        "\n",
        "# Legend\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n",
        "assert 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAT_3LWxI47q"
      },
      "source": [
        "## **Implementing a Linear Model in Torch for Harmonic Series**\n",
        "\n",
        "The harmonic series can be employed to approximate the Morse function, providing an alternative approach with certain advantages. Here's a brief overview before coding it in Torch:\n",
        "\n",
        "- **Morse Function**:\n",
        "  - Represented as $E_{Morse} = D_{e}\\left (1-\\exp^{-\\alpha(r-r_0)} \\right )^2$.\n",
        "  - Here, $\\alpha = \\sqrt{k/2D_{e}}$, with $D_e$, $r$, and $r_0$ being specific parameters of the system.\n",
        "\n",
        "- **Harmonic Series**:\n",
        "  - Expressed as $E_{harmonic} = \\sum_{\\ell=1}^p \\frac{k_\\ell}{r^{\\ell}}$.\n",
        "  - Unlike polynomials, it approaches zero as $r \\to \\infty$, making it suitable for certain types of approximations.\n",
        "  - In this context, it is essentially a linear model over the terms $[r, r^1, \\cdots, r^\\ell]$.\n",
        "\n",
        "- **Linear Model in Torch**:\n",
        "  - In PyTorch, $E_{harmonic}$ can be represented as a linear model, where $k_\\ell$ are the linear weights.\n",
        "  - The model will be constructed using a series of powers of $r$ as features, and the weights will be learned to best approximate the Morse function.\n",
        "\n",
        "The implementation in Torch involves defining the model structure, setting up the loss function and optimizer, and then training the model to learn the weights $k_\\ell$. This approach demonstrates the versatility of linear models and the power of PyTorch in modeling complex functions.\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <style>\n",
        "        .centered-image {\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 50%;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<!-- <a href=\"https://www-sop.inria.fr/members/Mathijs.Wintraecken/Mathematica.html\" target=\"_blank\">\n",
        "    <img src=\"https://www-sop.inria.fr/members/Mathijs.Wintraecken/Mathematica/MorseDecompositionF4.gif\"\n",
        "         alt=\"An animated explanation of Morse theory\"\n",
        "         class=\"centered-image\">\n",
        "</a>\n",
        "\n",
        "<br>\n",
        "<figcaption align = \"center\"><b>Figure 5 - An animated explanation of Morse theory. Animation by Mathijs Wintraecken.</b></figcaption> -->\n",
        "\n",
        "<!-- </body>\n",
        "</html>\n",
        "\n",
        "> Learn more about the [**Morse function**](https://www.tandfonline.com/doi/full/10.1080/08927020903433739). This paper discusses the use of Gradient fit functions for potential energy surfaces using a harmonic series. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Diagram**\\\n",
        "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/Linear_model_diagram.png\"  width=\"350\" height=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRrtz0kmgqEC"
      },
      "outputs": [],
      "source": [
        "# Define the Harmonic Series class\n",
        "class HarmonicSeries(nn.Module):\n",
        "    def __init__(self, ell):\n",
        "        super().__init__()\n",
        "        self.ell = ell\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.div(1, x)\n",
        "        R = x\n",
        "        for i in range(2, self.ell + 1):\n",
        "            r_l = x.pow(i)\n",
        "            R = torch.column_stack((R, r_l))\n",
        "        # Ensure that R has self.ell columns\n",
        "        return R[:, :self.ell]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR_SLGyQZAGh"
      },
      "outputs": [],
      "source": [
        "# Define the Linear Harmonic Model class\n",
        "class LinearHarmonicModel(nn.Module):\n",
        "    def __init__(self, ell):\n",
        "        super().__init__()\n",
        "        self.ell = ell\n",
        "        self.harmonic = HarmonicSeries(self.ell)\n",
        "        self.linear = nn.Linear(self.ell, 1)\n",
        "        nn.init.zeros_(self.linear.weight)  # Initialize weights to zeros\n",
        "\n",
        "    def forward(self, x):\n",
        "        r = self.harmonic(x)\n",
        "        y = self.linear(r)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5RpctgyRd8v"
      },
      "source": [
        "## **Fitting the PES for N2 (again)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4V1UYTlZEKT",
        "outputId": "0e870a4a-13e3-4928-cf27-fda0072a0f55"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "data_url = \"https://github.com/RodrigoAVargasHdz/CHEM-4PB3/raw/main/Course_Notes/data/PES_N2\"\n",
        "data = pd.read_csv(data_url)\n",
        "X, y = data['R'].to_numpy(), data['Energy'].to_numpy()\n",
        "X = torch.from_numpy(X).unsqueeze(1).float()\n",
        "y = y - np.min(y)\n",
        "y = torch.from_numpy(y).unsqueeze(1).float()\n",
        "\n",
        "\n",
        "print(\"Shape X -> \", X.shape)\n",
        "print(\"Len X -> \", len(X))\n",
        "\n",
        "print(\"Shape Y -> \", y.shape)\n",
        "print(\"Len Y -> \", len(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oOpWMHRLfF8"
      },
      "source": [
        "> Let's test our model before we starting fitting it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "2KWvU_kKLiOa",
        "outputId": "6419ff7a-51ed-42a7-d2e0-d46680bfed31"
      },
      "outputs": [],
      "source": [
        "# Test our model\n",
        "model_test = LinearHarmonicModel(10)\n",
        "loss_test = nn.MSELoss()\n",
        "y_pred_test = model_test(X).detach()\n",
        "print(\"Loss:\" ,loss_test(y_pred_test,y))\n",
        "print(\"Parameters:\")\n",
        "print(f\"  Weight: {model_test.linear.weight}\")\n",
        "print(f\"  Bias: {model_test.linear.bias}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(X.numpy(), y.numpy(), label='Actual')\n",
        "plt.plot(X.numpy(), model_test(X).detach().numpy(), label='Predicted')\n",
        "plt.xlabel('R(N-N)')\n",
        "plt.ylabel('Energy')\n",
        "# plt.xlim(0.5,3.)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V21rfgtJZHhn"
      },
      "outputs": [],
      "source": [
        "# Initialize the model with the correct number of features\n",
        "ell_value = 10\n",
        "model = LinearHarmonicModel(ell_value)  # Set ell to match the number of input features\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVtcRDYHZKVB",
        "outputId": "0941d2db-2919-477d-ddea-c27aba794106"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "training_iter = 2000\n",
        "loss_trajectory = []\n",
        "\n",
        "l2_lambda = 0.1\n",
        "for itr in range(1, training_iter):\n",
        "    optimizer.zero_grad()  # Clear gradients at the start of each iteration\n",
        "\n",
        "    output = model(X)  # Use the actual data for training\n",
        "    loss_val = loss_fn(output, y)\n",
        "\n",
        "    # Regularization term (L1 norm)\n",
        "    l_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "    loss_val += l2_lambda * l_norm\n",
        "\n",
        "    loss_val.backward()  # Perform backpropagation\n",
        "    optimizer.step()  # Update model parameters\n",
        "\n",
        "    if itr % 10 == 0:\n",
        "        print(f'Iteration = {itr}, Loss = {loss_val.item()}')\n",
        "\n",
        "    # Optionally, you can store the loss for later analysis\n",
        "    loss_trajectory.append(loss_val.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "77ou4dxxZPu4",
        "outputId": "ae2174c6-2294-4093-98c9-c2ce4d7414ce"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(X.numpy()[::2], y.numpy()[::2], marker='s', markersize=5, label='DFT')\n",
        "\n",
        "X_grid = torch.linspace(X.min(), X.max(), 2000)\n",
        "X_grid = X_grid.unsqueeze(-1)\n",
        "plt.plot(X_grid.detach().numpy(), model(X_grid).detach().numpy(), label='Harmonic Series')\n",
        "plt.xlabel('R(N-N)', fontsize=15)\n",
        "plt.ylabel('Energy', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWaXKhgZRy7N"
      },
      "source": [
        "## **Next Week**\n",
        "The Non-Linear Model: $f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x},\\mathbf{w}') = \\sum_i w_i \\phi_i(\\mathbf{x},\\mathbf{w}')$.\n",
        "\n",
        "> Optimizing the non-linear parameters $\\mathbf{w}'$.\n",
        "\n",
        "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/nonLinear_model_diagram.png\"  width=\"350\" height=\"300\">"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
