{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywr4UzTFMns3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1XwilcXbk5fAI1tGxzBvR6PzWX0-NFk6r?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlmObDC0aVkz"
      },
      "source": [
        "# **Week 7 - Introduction to Activation Functions.**\n",
        "\n",
        "## **Abstract**\n",
        "\n",
        "1. **Introduction to Activation Functions.**: Activation functions in neural networks, such as sigmoid, ReLU, and softmax, apply non-linear transformations to inputs, enabling the network to capture complex data patterns and behaviors.\n",
        "\n",
        "2. **Introduction to Non-Linear Layers**: Non-linear layers in neural networks transform linear inputs into non-linear outputs using activation functions, enabling the network to handle complex, non-linear relationships in data.\n",
        "\n",
        "\n",
        ">### **References: Essential Resources for Further Learning**\n",
        ">\n",
        ">- **PyTorch**: [Official Documentation](https://pytorch.org/docs/stable/index.html)\n",
        ">- **Activation Functions in Neural Networks**: [Online Course](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
        ">- **Activation Functions in Neural Networks [12 Types & Use Cases]**: [Blog](https://www.v7labs.com/blog/neural-networks-activation-functions)\n",
        "\n",
        "\n",
        "Feel free to explore these resources to deepen your understanding of data visualization, data management, and computational tools in Chemistry.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwb_Op0tNTXk"
      },
      "source": [
        "## **Beyond Linear Models - Introduction to Internal Parameters**\n",
        "\n",
        "\n",
        "In extending the linear model to a **non-linear model**, the function $f(\\mathbf{x})$ becomes dependent on both external and internal parameters. This can be dissected as follows:\n",
        "\n",
        "- **Non-Linear Model Representation**:\n",
        "  - The model is now expressed as \n",
        "    $$\n",
        "    f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x},\\mathbf{w}') = \\sum_i w_i \\phi_i(\\mathbf{x},\\mathbf{w}').\n",
        "    $$\n",
        "  - In this representation, $\\phi(\\mathbf{x},\\mathbf{w}')$ indicates a non-linear map to a new **feature representation**, which also depends on internal parameters $\\mathbf{w}'$.\n",
        "\n",
        "- **Internal Parameter Optimization**:\n",
        "  - The model includes optimization of the non-linear parameters $\\mathbf{w}'$.\n",
        "  - This introduces an additional layer of complexity compared to the linear model, as $\\mathbf{w}'$ need to be adjusted along with $\\mathbf{w}$.\n",
        "\n",
        "Let's assume $\\phi(\\mathbf{x},\\mathbf{w}')$ is another linear model,\\\n",
        "$\\phi(\\mathbf{x},\\mathbf{w}') = \\mathbf{z} = [z_0,z_1,\\cdots,z_\\ell]$, where $\\ell$ is the \"new\" number of features.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf_kqzG3R_Um"
      },
      "outputs": [],
      "source": [
        "# Function\n",
        "def f(x):\n",
        " return -(1.4 - 3.0 * x) * torch.sin(18.0 * x)\n",
        "\n",
        "def get_data(n_batch=200):\n",
        "    # X = torch.randn((n_batch,1))\n",
        "    X = torch.distributions.uniform.Uniform(-0.01,1.).sample([n_batch,1])\n",
        "    y = f(X)\n",
        "    return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O42yJ6yaZkpH"
      },
      "outputs": [],
      "source": [
        "# Model definition\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(1, 100),\n",
        "  nn.Linear(100, 100),\n",
        "  nn.Linear(100, 75),\n",
        "  nn.Linear(75, 1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF2jNpyBSCJd"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def training(model,training_iter=500, n_batch=50, lr=0.05, feedback_interval=5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    model.train()\n",
        "\n",
        "    for itr in range(1, training_iter + 1):\n",
        "        X, y_true = get_data(n_batch)\n",
        "        output = model(X)\n",
        "        loss_val = loss_fn(output, y_true)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if itr % feedback_interval == 0:\n",
        "            print(f'Iteration = {itr}, Loss = {loss_val.item():.4f}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyjXc6K_R8tH"
      },
      "outputs": [],
      "source": [
        "def plot_model_performance(model, n_samples=75, n_grid_points=5000):\n",
        "    plt.clf()\n",
        "    X, y = get_data(n_samples)\n",
        "    X_grid = torch.linspace(0., 1., n_grid_points).unsqueeze(1)\n",
        "\n",
        "    plt.scatter(X.detach().numpy(), y.detach().numpy(), label='Data')\n",
        "    plt.plot(X_grid.detach().numpy(), f(X_grid).detach().numpy(), ls='--', c='k', label=r'$f(x)$')\n",
        "    plt.plot(X_grid.detach().numpy(), model(X_grid).detach().numpy(), c='red', label=r'$NN(x)$')\n",
        "\n",
        "    plt.ylabel(r'$f(x)$', fontsize=12)\n",
        "    plt.xlabel(r'$x$', fontsize=12)\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pHbJLlm9SPef",
        "outputId": "1489cc72-cde3-4514-868f-2df481a7b3ba"
      },
      "outputs": [],
      "source": [
        "model_trained = training(model)\n",
        "plot_model_performance(model_trained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VTk3B-7TEfG"
      },
      "source": [
        "### **Non-Linear Layers**\n",
        "\n",
        "Consider the composition of two linear models:\n",
        "\n",
        "$$\n",
        "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\mathbf{W}_2^\\top (\\mathbf{x}\\mathbf{W}^\\top_1) = \\mathbf{z}\\mathbf{W}^\\top_2\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "In this expression, $\\mathbf{W}_1$ and $\\mathbf{W}_2$ are the weight matrices of the two linear models. The function $f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2})$ can be rewritten to highlight the composition of these models:\n",
        "\n",
        "$$\n",
        "f(x,\\{\\mathbf{W}\\}_{\\ell=1}^{2}) = \\mathbf{W}_2^\\top \\phi(\\mathbf{W}_1, \\mathbf{x})\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Where the function $\\phi(\\mathbf{W}_1, \\mathbf{x})$ is defined as the product of the input $\\mathbf{x}$ and the transpose of the first weight matrix $\\mathbf{W}_1$:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\phi(\\mathbf{W}_1, \\mathbf{x}) = \\mathbf{x}\\mathbf{W}^\\top_1\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t75NXzN8Tl1H"
      },
      "source": [
        "## **Introducting Activation functions**\n",
        "\n",
        "Let's revisit the structure of Nueral Networks to understand activation functions:\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <style>\n",
        "        .centered-image {\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 50%;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<a href=\"https://www.geeksforgeeks.org/activation-functions/\" target=\"_blank\">\n",
        "    <img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/33-1-1.png\"\n",
        "         alt=\"Varied Initial Conditions for Gradient Descent\"\n",
        "         class=\"centered-image\">\n",
        "</a>\n",
        "\n",
        "<br>\n",
        "<figcaption align = \"center\"><b>Figure 1 - Activation Functions. Figure by\n",
        "Vineet Joshi.</b></figcaption>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "<br>\n",
        "\n",
        "An **activation function** (also known as \"transfer functions\") in a neural networks define how the weighted sum of the input (**Wnj**) is transformed into an output from a node or nodes in a layer of the network.\n",
        "\n",
        "<br>\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <style>\n",
        "        .centered-image {\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 50%;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<a href=\"https://prabhakar-rangarao.medium.com/activation-functions-9020acfa80b6\" target=\"_blank\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1358/1*uUzr1DsZm5P6IqRXIlpfhQ.gif\"\n",
        "         alt=\"Activation Functions Animated\"\n",
        "         class=\"centered-image\">\n",
        "</a>\n",
        "\n",
        "<br>\n",
        "<figcaption align = \"center\"><b>Figure 2 - Activation Functions Animated. Figure by\n",
        "Prabhakar Rangarao.</b></figcaption>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "<br>\n",
        "\n",
        "Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function.\n",
        "\n",
        "<br>\n",
        "\n",
        "* Hyperbolic tangent\n",
        "$$\n",
        "tanh(x) = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)}\n",
        "$$\n",
        "\n",
        "* Sigmoid\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{1}{1+ \\exp(-x)}\n",
        "$$\n",
        "\n",
        "* ReLU\n",
        "$$\n",
        "\\text{ReLU}(x) = \\max(0,x)\n",
        "$$\n",
        "\n",
        "* Leaky RLU\n",
        "$$\n",
        "\\text{LeakyReLU}(x) = \\max(0,x) + \\beta*\\min(0,x)\n",
        "$$\n",
        "\n",
        "* SiLU\n",
        "$$\n",
        "\\text{SiLU}(x) = x * \\sigma(x)\\\\\n",
        "\\sigma(x) = \\frac{1}{1+\\exp(-x)}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFuAG5vGYrAM"
      },
      "outputs": [],
      "source": [
        "x = torch.linspace(-5,5,1000)\n",
        "\n",
        "act_tanh = nn.Tanh()\n",
        "y_tanh = act_tanh(x)\n",
        "\n",
        "act_sigmoid = nn.Sigmoid()\n",
        "y_sigmoid = act_sigmoid(x)\n",
        "\n",
        "act_relu = nn.ReLU()\n",
        "y_relu = act_relu(x)\n",
        "\n",
        "act_lrelu = nn.LeakyReLU(0.1)\n",
        "y_lrelu = act_lrelu(x)\n",
        "\n",
        "act_silu = nn.SiLU()\n",
        "y_silu = act_silu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "knZdWa0vYriB",
        "outputId": "924081d0-2d79-49c2-d1bd-1ae91683c65c"
      },
      "outputs": [],
      "source": [
        "xnp = x.detach().numpy()\n",
        "plt.plot(xnp,y_tanh.detach().numpy(),label='Tanh')\n",
        "plt.plot(xnp,y_sigmoid.detach().numpy(),label='Sigmoid')\n",
        "plt.plot(xnp,y_relu.detach().numpy(),label='ReLU')\n",
        "plt.plot(xnp,y_lrelu.detach().numpy(),label='Leaky ReLU')\n",
        "plt.plot(xnp,y_silu.detach().numpy(),label='SiLU')\n",
        "plt.xlabel('x',fontsize=15)\n",
        "plt.ylabel('activation function',fontsize=15)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qS1axDDY0Ug"
      },
      "source": [
        "## **In Class Activity - Discuss one of the Activation Functions**\n",
        "\n",
        "**Diagram**\\\n",
        "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/MLP_diagram.png\"  width=\"400\" height=\"300\">\n",
        "\n",
        "Chose one of the activation functions discussed and use it in your linear model. Which function works the best?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "ANgR_9izZaj3",
        "outputId": "bbd9bf6e-fc83-4a02-c142-6d3554284c67"
      },
      "outputs": [],
      "source": [
        "# Code here\n",
        "# Define a model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 1)\n",
        ")\n",
        "\n",
        "model = training(model)\n",
        "X_grid = torch.linspace(0., 1., 5000).unsqueeze(1)\n",
        "plt.clf()\n",
        "X, y = get_data(25)\n",
        "print(X.shape, y.shape, model(X).shape)\n",
        "plt.scatter(X.detach().numpy(), y.detach().numpy(), label='Data')\n",
        "plt.plot(X_grid.detach().numpy(), f(X_grid).detach().numpy(),\n",
        "         ls='--', c='k', label=r'$f(x)$')\n",
        "plt.plot(X_grid.detach().numpy(), model(\n",
        "    X_grid).detach().numpy(), c='red', label=r'$NN(x)$')\n",
        "plt.ylabel(r'$f(x)$', fontsize=12)\n",
        "plt.xlabel(r'$x$', fontsize=12)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HLmioAOZ2or"
      },
      "source": [
        "Work in a small groups and discuss the following.\n",
        "1. How many layers we need?\n",
        "2. What is the *best* activation function?\n",
        "\n",
        "\n",
        "# Extra\n",
        "\n",
        "Go to the following [link](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.03345&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) and try to solve all the different tasks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradient Descent ##\n",
        "\n",
        "In the previous tutorial we saw that we could gradient based methods to optimize the parameters of a ML model. <br>\n",
        "If we give a close look at the ```training()``` function, we can see that at each iteration (step), we used the complete training data to updated the parameters. \n",
        "$$\n",
        "\\boldsymbol{\\Theta}_{i+1} = \\boldsymbol{\\Theta}_{i} - \\eta \\nabla_{\\boldsymbol{\\Theta}} {\\cal L}(\\boldsymbol{\\Theta}_{i})\\\\\n",
        "\\boldsymbol{\\Theta}_{i+1} = \\boldsymbol{\\Theta}_{i} - \\text{optimizer}(\\nabla_{\\boldsymbol{\\Theta}} {\\cal L}(\\boldsymbol{\\Theta}_{i}))\\\\\n",
        "$$\n",
        "In this general picture, the $\\texttt{optimizer}$ dictates the policy to update the parameters given the values of the gradient.\n",
        "\n",
        "### Torch optimizers ###\n",
        "\n",
        "There are many optimizers available in ```torch```; [link](https://pytorch.org/docs/stable/optim.html) <br>\n",
        "The most common ones are, \n",
        "1. [```Adam (Adaptive Moment Estimation)```](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwi2u8j6xNOEAxXuJNAFHfSfA_QQFnoECBEQAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1412.6980&usg=AOvVaw2YQSuFAZFEf23o6Y5-8oMH&opi=89978449) (168400 citations)\n",
        "2. [```SGD (Stochastic Gradient Descent)```](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (~1B citations)\n",
        "3. [```RMSProp (Root Mean Square Propagation)```](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) (does not have a paper, but this website has been cited 1876)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### minibatch SGD ###\n",
        "A full optimization when the training data is large, could be **impossible**.<br>\n",
        "In regression, one way to add \"noisy\" in the optimization/training is by doing a sub-sampling of the training dataset. <br>\n",
        "\n",
        "1. At each iteration we randomly selecte a *minibatch* of the training data, ${\\cal B}_t$ as the training dataset.\n",
        "2. Using ${\\cal B}_t$, we compute the gradient of the average loss/error function.\n",
        "3. Update the parameters, \n",
        "   $$\n",
        "    \\boldsymbol{\\Theta}_{i+1} = \\boldsymbol{\\Theta}_i - \\text{optimizer}(\\nabla_{\\boldsymbol{\\Theta}} \\tilde{{\\cal L}}(\\boldsymbol{\\Theta}_{i}; {\\cal B}_t))\n",
        "   $$\n",
        "\n",
        "Notes: \n",
        "1. **hyperparameters**: *minibatch* size and $eta$ are user defined $\\to$ *student descent*.\n",
        "2. The quality of the model is assessed using a *validation* set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return sample, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset\n",
        "X, y = get_data(50)\n",
        "dataset = CustomDataset(X, y)\n",
        "\n",
        "# Create a DataLoader to batch data\n",
        "batch_size = 10\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Training loop\n",
        "def training_minibatch(model,dataloader,lr=1E-3,epochs=50):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Using tqdm to show progress\n",
        "        loss_epoch = []\n",
        "        loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=True)\n",
        "        for batch_idx, (data, y_tr) in loop:\n",
        "            # Simulate training steps\n",
        "            # 1. Zero gradients (if using gradient-based optimization)\n",
        "            optimizer.zero_grad()\n",
        "            # 2. Compute prediction (forward pass)\n",
        "            output = model(data)\n",
        "            # 3. Compute loss\n",
        "            loss = loss_fn(output, y_tr)\n",
        "            # 4. Backpropagation (backward pass)\n",
        "            loss.backward()\n",
        "            # 5. Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_epoch.append(loss.detach())\n",
        "        # Update progress bar description\n",
        "            loop.set_description(\n",
        "                f'Epoch {epoch}, loss = {np.array(loss_epoch).mean():.4f}')\n",
        "            loop.set_postfix(batch=batch_idx+1)\n",
        "            # print(np.array(loss_epoch).mean())\n",
        "            \n",
        "            # minibatch plotting\n",
        "            # plt.figure(0)\n",
        "            # plt.clf()\n",
        "            # plt.title(f'batch: {batch_idx}')\n",
        "            # plt.scatter(X.detach().numpy(), y.detach().numpy(), label='Full Data')\n",
        "            # plt.scatter(data.detach(), y_tr.detach(),label='minibatch')\n",
        "            # plt.legend(loc=2)\n",
        "            # plt.ylabel(r'$f(x)$', fontsize=12)\n",
        "            # plt.xlabel(r'$x$', fontsize=12)\n",
        "            # plt.ylim(-1.6,1.5)\n",
        "            # plt.pause(0.1)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = training_minibatch(model,dataloader)\n",
        "\n",
        "X_grid = torch.linspace(0., 1., 5000).unsqueeze(1)\n",
        "plt.clf()\n",
        "X, y = get_data(25)\n",
        "print(X.shape, y.shape, model(X).shape)\n",
        "plt.scatter(X.detach().numpy(), y.detach().numpy(), label='Data')\n",
        "plt.plot(X_grid.detach().numpy(), f(X_grid).detach().numpy(),\n",
        "         ls='--', c='k', label=r'$f(x)$')\n",
        "plt.plot(X_grid.detach().numpy(), model(\n",
        "    X_grid).detach().numpy(), c='red', label=r'$NN(x)$')\n",
        "plt.ylabel(r'$f(x)$', fontsize=12)\n",
        "plt.xlabel(r'$x$', fontsize=12)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
