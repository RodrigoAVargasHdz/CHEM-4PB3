{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpHmTbShMO89"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/RodrigoAVargasHdz/CHEM-4PB3/blob/w2024/Course_Notes/Week%203/Week_3_Intro_Linear_Models.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# **Week 3 - Introduction to Linear Models**\n",
        "\n",
        "## **Abstract**\n",
        "\n",
        "1. **Brief Overview of Parameter Generation**: Creating an array of random integers and adding Gausian Noise to data using **NumPy's** `Uniform` function.  \n",
        "\n",
        "2. **Animations on Google Colab**: Introducing the `IPython.display` to create gifs using individual frames. These can be used to visualize Kinetic Models in Python.\n",
        "\n",
        "3. **Linear and Polynomial Models**: The use of Linear and Polynomial models to predict data, inspecting the advantages and distadvantages of each model. Introducing the mathmatical structure of these models.\n",
        "\n",
        "4. **Overfitting and Underfitting**: Overfitting  occurs when a model learns the training data too well, capturing noise and irrelevant details, which leads to poor generalization to new data. Underfitting, occurs when a model is too simple to capture the underlying patterns in the data, resulting in a lack of accuracy both on the training and test data.\n",
        "\n",
        "5. **Understanding Lambda and Regularization**: To avoid overfitting or underfitting data, lambda are introduced as critical elements in machine learning. Introducing the magnitudes of the coefficient to optimize regularization.\n",
        "\n",
        "\n",
        ">## **References: Essential Resources for Further Learning**\n",
        ">\n",
        ">1. **NumPy Random Tutorial**: [Tutorial](https://numpy.org/doc/stable/reference/random/index.html)\n",
        "2. **Creating Animations with Matplotlib**: [Tutorial](https://towardsdatascience.com/animations-with-matplotlib-d96375c5442c)\n",
        "3. **Linear Regression in Python**: [Tutorial](https://realpython.com/linear-regression-in-python/)\n",
        "4. **Overfitting and Underfitting in Machine Learning**: [Article](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)\n",
        "5. **Regularization in Machine Learning**: [Tutorial](https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net)\n",
        "\n",
        "\n",
        "Feel free to explore these resources to deepen your understanding of data visualization, data management, and computational tools in Chemistry.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3SPUNHjwhHf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML, Image # For GIF\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADCJ63YVMnhx"
      },
      "source": [
        "## **Understanding Linear Models in Python - Animation**\n",
        "\n",
        "To visualize Linear Kinetic Models in Python, we can generate random data using **NumPy's** `Uniform` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05GPnAEhwiwc"
      },
      "outputs": [],
      "source": [
        "# generate random data over f(x) = sin(x) + x - 1\n",
        "def get_data(N):\n",
        "    x = np.linspace(-1.,1.,N) #This creates an array x of N linearly spaced values between -1 and 1.\n",
        "    y = np.sin(.5*x) + x -1.\n",
        "    y = y + np.random.uniform(low = 0.,high=0.5,size=x.shape) #Adds random noise to each y value.\n",
        "    return x,y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear models 101:\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x) &= m x + b\n",
        "=  \\sum_{i=0}^{d} w_i x_i = \\begin{bmatrix}\n",
        "w_0 & w_1 & \\cdots & w_p \\\\\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        " 1 \\\\\n",
        " x_1 \\\\\n",
        " \\vdots \\\\\n",
        " x_d\n",
        "\\end{bmatrix}\n",
        "\\end{align} \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "jdP4jTIrwlxV",
        "outputId": "a1dbef26-6998-4c87-ed4f-68f3adc27315"
      },
      "outputs": [],
      "source": [
        "def model(x,params):\n",
        "    m,b = params #tuple\n",
        "    return m*x + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3uCgKm4wt-r"
      },
      "source": [
        "### **Generating Random Parameters**\n",
        "\n",
        "*   **Purpose:** The purpose of this function is to generate a 2D matrix of random numbers. The dimensions of this array are $m x 2$, meaning there are m rows and 2 columns.\n",
        "\n",
        "- $$[m,b] \\sim U([-2,2])$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ_UYrm_wwX3"
      },
      "outputs": [],
      "source": [
        "#random parameters\n",
        "def get_random_params(m):\n",
        "    theta_random = np.random.uniform(low=-2.,high=2.,size=(m,2))\n",
        "    return theta_random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Funcitons in Python**\n",
        "\n",
        "1. Build-in functions: `min(x)`\n",
        "2. User-Defined Functions: \n",
        "   ```python\n",
        "      def f_min(x): \n",
        "         return np.min(x)\n",
        "   ```\n",
        "3. lambda functions: \n",
        "   ```python\n",
        "   lambda x: np.min(x)\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise**<br>\n",
        "Write a lambda function for a linear model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXT7WNatw1C3"
      },
      "source": [
        "## **Figure Per Frame**\n",
        "\n",
        "\n",
        "> The model will plot a series of linear models based on the parameters.\n",
        "\n",
        "> Loops over each pair of parameters **`(m, b)`** in `theta_rnd`.\n",
        "For each pair, it calls `plot_figure_frame` to plot the data and the linear model based on those parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> This graph shows the datapoints generated. Using Python and Linear Regression, we can fit a model (a line of best fit) to the datapoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = get_data(25)\n",
        "plt.scatter(x, y, label='data')\n",
        "plt.xlabel(r'$x$', fontsize=18)\n",
        "plt.ylabel(r'$f(x)$', fontsize=18)\n",
        "plt.ylim(-3., 2.)\n",
        "plt.legend()\n",
        "# plt.savefig('Figures/data.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_figure_frame(data, params, i):\n",
        "    m, b = params\n",
        "    x, y = data\n",
        "    def f(x, m, b): return m*x + b\n",
        "\n",
        "    x_grid = np.linspace(-1., 1., 100)\n",
        "    y_pred = f(x_grid, m, b)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.clear()\n",
        "    ax.scatter(x, y, label='data')\n",
        "    ax.plot(x_grid, y_pred, color='k', label='model')\n",
        "    ax.text(0.2, -2.5, 'm=%.2f, b=%.2f' % (m, b), fontsize=15)\n",
        "    ax.legend(loc=1)\n",
        "    ax.set_xlabel(r'$x$', fontsize=18)\n",
        "    ax.set_ylabel(r'$f(x)$', fontsize=18)\n",
        "    ax.set_ylim(-3., 2.)\n",
        "    # plt.savefig('Figures/linear_model_%s.png'%(i))\n",
        "    plt.draw()\n",
        "    plt.pause(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i80tlPyxzskR",
        "outputId": "917a7047-a8b5-4e88-e897-384837602714"
      },
      "outputs": [],
      "source": [
        "theta_rnd = get_random_params(2)\n",
        "x, y = get_data(25)\n",
        "for i, p in enumerate(theta_rnd):\n",
        "    plot_figure_frame((x,y),p,i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aTsZK-C08uL"
      },
      "source": [
        "## **Animation**\n",
        "\n",
        "> To animate the Linear model in Google Colab, the [`IPython.display`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html) library can convert frames to gifs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "31pK2DzX1FGS",
        "outputId": "fd42bb76-66ed-4c54-996c-071b3995db49"
      },
      "outputs": [],
      "source": [
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "\n",
        "x,y = get_data(25)\n",
        "theta_rnd = get_random_params(20)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,5))\n",
        "# plt.figure(facecolor='white')\n",
        "\n",
        "ax.set_xlabel(r'$x$', fontsize=18)\n",
        "ax.set_ylabel(r'$f(x)$', fontsize=18)\n",
        "ax.set_ylim(-3., 2.)\n",
        "ax.set_xlim(-1.1, 1.1)\n",
        "ax.scatter(x,y,label='data')\n",
        "\n",
        "# line1, = ax.plot([], [], ms=20, label='data')\n",
        "line2, = ax.plot([], [], ms=20, color='k', label='model')\n",
        "txt2 = ax.text(0.2,-2.5, '',fontsize=15)\n",
        "ax.legend(loc=1)\n",
        "\n",
        "def drawframe(n):\n",
        "    params = theta_rnd[n]\n",
        "    m, b = params\n",
        "    def f(x, m, b): return m*x + b\n",
        "\n",
        "    x_grid = np.linspace(-1., 1., 100)\n",
        "    y_pred = f(x_grid, m, b)\n",
        "\n",
        "    # line1.set_data(x,y)\n",
        "    line2.set_data(x_grid,y_pred)\n",
        "    txt2.set_text('m=%.2f, b=%.2f'%(m,b))\n",
        "\n",
        "    return (line2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGgMO8HNRo5h"
      },
      "source": [
        "> The original plotted data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "nuwpvbzO1Mxf",
        "outputId": "bdcc58e4-74e0-4511-b34f-ad9123e621c9"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "# blit=True re-draws only the parts that have changed.\n",
        "anim = animation.FuncAnimation(\n",
        "    fig, drawframe, frames=19, interval=200, blit=False,)\n",
        "\n",
        "# Save as GIF\n",
        "anim.save('animation.gif')  # writer='pillow'\n",
        "\n",
        "# play animation\n",
        "HTML(anim.to_html5_video())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0V4mzWILekw"
      },
      "source": [
        "## **Polynomials Models are linear Models**\n",
        "\n",
        "Polynomial models are a class of regression models that use polynomial functions to fit a relationship between a dependent variable and **one or more independent variables**. Unlike linear models, which are constrained to a straight line, polynomial models can **fit data with curves and complex** relationships, making them more flexible for a wide range of datasets.\n",
        "\n",
        "\n",
        "- **General Form**: The general form of a polynomial model is given by:\n",
        "  $$\n",
        "    f(x) = w_0 + w_1x + w_2x^2 + \\ldots + w_px^p = \\begin{bmatrix}\n",
        "    w_0 & w_1 & \\cdots & w_p \\\\\n",
        "    \\end{bmatrix} \\begin{bmatrix}\n",
        "    1 \\\\\n",
        "    x^1 \\\\\n",
        "    \\vdots \\\\\n",
        "    x^p\n",
        "      \\end{bmatrix}\n",
        "  $$\n",
        "    where:\n",
        "    - $x$ is the independent variable.\n",
        "    - $w_0, w_1, \\ldots, w_n$ are the coefficients of the model.\n",
        "    - $n$ is the degree of the polynomial.\n",
        "\n",
        "- **Degree of the Polynomial**:\n",
        "  - The degree $n$ of the polynomial determines the curve's complexity.\n",
        "  - A degree of 1 corresponds to a linear model (straight line).\n",
        "  - Higher degrees (2 for quadratic, 3 for cubic, etc.) allow for more complex curves.\n",
        "\n",
        "\n",
        "- **Flexibility**:\n",
        "  - Polynomial models are more flexible than linear models, able to fit data with curves and **non-linear relationships**.\n",
        "\n",
        "\n",
        "- **Overfitting Concern**:\n",
        "  - Caution is needed to avoid overfitting, especially with high-degree polynomials.\n",
        "  - Overfitting occurs when the model becomes too complex, fitting the noise in the data rather than the underlying trend.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "9n34K19ymdD_",
        "outputId": "041e9c46-41dd-4cc9-f3b1-260789aa8cca"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "\n",
        "# New function to generate data\n",
        "def get_data(N):\n",
        "    x = np.linspace(-2, 2, N)\n",
        "    y = np.sin(x**3) * np.cos(x) - x**3\n",
        "    y += np.random.normal(scale=1, size=x.shape)  # Adding Gaussian noise\n",
        "    return x, y\n",
        "\n",
        "# Generate data\n",
        "x, y = get_data(100)\n",
        "\n",
        "# Set up the figure, the axis, and the plot element\n",
        "fig, ax = plt.subplots()\n",
        "# ax.set_xlim((min(x), max(x)))\n",
        "# ax.set_ylim((min(y)-1, max(y)+1))\n",
        "\n",
        "# Plot the data points\n",
        "ax.scatter(x, y, color='red')\n",
        "\n",
        "\n",
        "\n",
        "# # Frame by frame\n",
        "# for pi in range(1,20,2): \n",
        "#     z = np.polyfit(x, y, pi)  # what is this?\n",
        "#     poly_model = np.poly1d(z)  # what is this?\n",
        "#     ax.plot(x, poly_model(x),label='p = %s' % pi)\n",
        "#     # break\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "    \n",
        "\n",
        "# Animation function\n",
        "# Initialization function: plot the background of each frame\n",
        "line, = ax.plot([], [], lw=2)\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    return (line,)\n",
        "def animate(i):\n",
        "    if i == 0:\n",
        "        return (line,)\n",
        "    z = np.polyfit(x, y, i) #what is this? \n",
        "    poly_model = np.poly1d(z) #what is this?\n",
        "    line.set_data(x, poly_model(x))\n",
        "    ax.set_title(f\"Polynomial Degree {i}\")\n",
        "    return (line,)\n",
        "\n",
        "# Call the animator\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                               frames=15, interval=200, blit=True)\n",
        "\n",
        "# Display the animation\n",
        "HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuhkYgNQLLie"
      },
      "outputs": [],
      "source": [
        "# polynomial model 1D\n",
        "def poly_pred(data_tr, deg, x_grid):\n",
        "    x, y = data_tr\n",
        "\n",
        "    #training \n",
        "    w = np.polyfit(x, y, deg)\n",
        "    poly_model = np.poly1d(w) \n",
        "    \n",
        "    #prediction\n",
        "    y_pred = poly_model(x_grid)\n",
        "    return y_pred,w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "PMUU4zC1LPLq",
        "outputId": "b95b0291-1634-4889-b730-c084b6a212a6"
      },
      "outputs": [],
      "source": [
        "x, y = get_data(10)\n",
        "data_tr = (x,y)\n",
        "x_grid = np.linspace(-2., 2., 200)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "w_ = [] \n",
        "p_ = np.arange(1, 13, 1,dtype=np.int32)\n",
        "for p in p_: # loop over different degrees\n",
        "    y_pred, w = poly_pred(data_tr, p, x_grid)\n",
        "    print(p,w)\n",
        "    plt.plot(x_grid,y_pred,label='p=%s'%p)\n",
        "    w_.append(np.pad(w, (0, 13-w.shape[0]),\n",
        "              mode='constant', constant_values=0))\n",
        "plt.scatter(x,y,s=75,label='data')\n",
        "plt.legend(fontsize=5)\n",
        "plt.xlabel(r'$x$',fontsize=18)\n",
        "plt.ylabel(r'$f(x)$',fontsize=18)\n",
        "# plt.savefig('Figures/polyfit_2.png',dpi=1800)\n",
        "\n",
        "fig, ax0 = plt.subplots(1, 1)\n",
        "c = ax0.pcolor(np.abs(np.array(w_)), edgecolors='k', linewidths=4)\n",
        "fig.colorbar(c, ax=ax0,label=r'$|w_i|$')\n",
        "ax0.set_xlabel(r'$w_i$')\n",
        "ax0.set_yticks(np.arange(p_.shape[0])+0.5, p_)\n",
        "ax0.set_ylabel('Poly degree')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3LJQk7-eTpd"
      },
      "source": [
        "### **Regularization and Overfitting - Introduction**\n",
        "\n",
        "In the last code snipped there was a `Ranking Warning` warning. A `Rank Warning` means the polynomial being fit to the data might not work well,  because the polynomial is too complex compared to the amount of data. In application, this can be avoided by:\n",
        "\n",
        "<br>\n",
        "\n",
        "1. **Conditioning of the Problem**: High-degree polynomials can cause numerical instability and poor conditioning, leading to large oscillations in the fit.\n",
        "\n",
        "2. **Data Scaling**: Scaling both `x` and `y` data to a smaller range can improve the condition of the polynomial fitting and potentially reduce numerical errors.\n",
        "\n",
        "3. **Regularization**: Regularization techniques like reducing the polynomial degree can limit model complexity, but are less straightforward in polynomial fitting compared to linear models.\n",
        "\n",
        "\n",
        "> It is important to condition and regularize the data to avoid **overfitting** or **underfitting** the polynominal.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<a href=\"https://github.com/RodrigoAVargasHdz/CHEM-4PB3\" target=\"_blank\">\n",
        "  <img src=\"https://raw.githubusercontent.com/RodrigoAVargasHdz/CHEM-4PB3/Volume-2/Assests/overfitting_underfitting_graphs.png\" alt=\"Visualizing LogP\" width=\"700\">\n",
        "</a>\n",
        "<br>\n",
        "</a>\n",
        "\n",
        "**Figure 3 - Illustrating Overfitting versus Underfitting. Image by Rudra Sondhi.**\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        ">#### **Learn More:**\n",
        "1. **What is Overfitting in Deep Learning**: [Educational Resource](https://www.v7labs.com/blog/overfitting)\n",
        "2. **Regularization in Machine Learning (with Code Examples)**: [Example Application](https://www.dataquest.io/blog/regularization-in-machine-learning/)\n",
        "3. **Polynomial Regression**: [Educational Resource](https://medium.com/analytics-vidhya/polynomial-regression-%EF%B8%8F-e0e20bfbe9d5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyLsfBwEn8nq"
      },
      "source": [
        "## **The Challenge of Overfitting**\n",
        "\n",
        "- **Risk in Flexibility**:  Higher-degree polynomials might fit the training data too precisely, capturing noise rather than the underlying trend, a phenomenon known as overfitting.\n",
        "- **Consequences**: An overfitted model performs poorly on new, unseen data, rendering it less effective for predictive purposes.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Regularization: A Key Solution**\n",
        "\n",
        "Regularization addresses the challenge of overfitting by introducing constraints into the model fitting process. It penalizes the complexity of the model, encouraging simpler models that are more likely to generalize well.\n",
        "\n",
        "- **Formula and Implementation**: The optimal parameters for a regularized polynomial model can be computed using the formula:\n",
        "  - $\\boldsymbol{\\theta}^{*}(\\lambda) = \\left ( \\mathbf{\\Phi}(\\mathbf{X})^{\\top} \\mathbf{\\Phi}(\\mathbf{X})  + \\lambda \\mathbb{I}\\right)^{-1} \\mathbf{\\Phi}(\\mathbf{X})^{\\top}\\mathbf{y}$, where $\\lambda$ is the regularization parameter.\n",
        "- **Balancing Act**: The regularization parameter $\\lambda$ plays a crucial role in balancing the model's complexity against its ability to generalize. It determines the extent to which the model's complexity is penalized, impacting both bias and variance.\n",
        "- **The parameter $\\boldsymbol{\\theta} \\$:** In the context of regularization are the coefficients of the model that are adjusted to minimize the penalty imposed by regularization.\n",
        "\n",
        "> **Library:** [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) from `Sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QVG1kfNLkfQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Define the linear model solver as per the provided code\n",
        "def linear_model_solver(data, deg, l):\n",
        "    x, y = data\n",
        "    poly = PolynomialFeatures(deg)\n",
        "    X = x[:, np.newaxis]  # reshape x to (N, 1)\n",
        "    Phi = poly.fit_transform(X)  # create polynomial features\n",
        "    Phit_Phi = Phi.T @ Phi\n",
        "    Phit_y = Phi.T @ y\n",
        "    lambda_I = l * np.eye(Phit_Phi.shape[0])\n",
        "    A = Phit_Phi + lambda_I\n",
        "    A_inv = np.linalg.inv(A)\n",
        "    w = A_inv @ Phit_y\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UzyVCCKLovE"
      },
      "outputs": [],
      "source": [
        "def f_poly(x, w, deg):\n",
        "    poly = PolynomialFeatures(deg)\n",
        "    X = x[np.newaxis].T\n",
        "    Phi = poly.fit_transform(X)\n",
        "    Phi = Phi[:, ::-1] # reverse column order (xixj, ..., 1)\n",
        "    y_pred = Phi @ w\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPSXbnVEp-i6"
      },
      "source": [
        "\n",
        ">**Polynomial Feature Transformation**:\n",
        "**Inputs**:\n",
        "- Dataset : `data`\n",
        "- Degree of the polynomial : `deg`\n",
        "- Regularization parameter `l` ( or $\\boldsymbol{\\lambda} \\$)\n",
        "\n",
        ">The model considers linear terms like **$x$** but also higher-degree terms  **$x^2$, $x^3$, $..., x^{\\text{deg}} $**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE1VA5IZLsmB"
      },
      "outputs": [],
      "source": [
        "x,y = get_data(7)\n",
        "l_ = np.array([0.,1E-4,1E-3,1E-2,1E-1,1.]) # lambda\n",
        "w_ = []\n",
        "deg = 7\n",
        "for l in l_: # loop over variuos lambda\n",
        "    w = linear_model_solver((x,y),deg,l)\n",
        "    w_.append(w) # list of optimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "MZ7pblSaLveX",
        "outputId": "9ebe4352-0ec5-4caf-f94d-dbdc32a00418"
      },
      "outputs": [],
      "source": [
        "plt.figure(0)\n",
        "cbar = plt.imshow(w_)\n",
        "cbar = plt.colorbar(cbar)\n",
        "cbar.set_label(r'magnitud of $\\theta_i$',fontsize=15)\n",
        "plt.ylabel(r'$\\lambda$',fontsize=15)\n",
        "lambda_ticks = [r'$0$', r'$10^{-4}$', r'$10^{-3}$',\n",
        "                r'$10^{-2}$', r'$10^{-1}$', r'$10^{1}$']\n",
        "plt.yticks(np.arange(len(l_)),lambda_ticks)\n",
        "theta_ticks = [r'$\\theta_{%s}$'%i for i in range(len(w_[0]))]\n",
        "plt.xticks(np.arange(len(w_[0])), theta_ticks,fontsize=10)\n",
        "\n",
        "#Set up the DataFrame\n",
        "df = pd.DataFrame(w_, index=l_, columns=[f'θ{i}' for i in range(deg + 1)])\n",
        "\n",
        "# Display the DataFrame\n",
        "df_styled = df.style.set_table_styles(\n",
        "    [{'selector': 'th', 'props': [('font-size', '12pt')]}]\n",
        ").set_caption(\"Coefficients of Polynomial Model for Different λ Values\").format(\"{:.2e}\")\n",
        "\n",
        "df_styled\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN4jO3jsr_1r"
      },
      "source": [
        "> As $\\boldsymbol{\\lambda}$ increases, the magnitudes of the coefficients ($\\boldsymbol{\\theta}$) typically decrease, which can be observed from the color changes in the heatmap. This effect is part of regularization's role in preventing overfitting by penalizing larger coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "eSwLLQX-Lz16",
        "outputId": "bb739c8e-c5d0-43b7-c891-ceccc9dcd284"
      },
      "outputs": [],
      "source": [
        "deg = 7\n",
        "for w,l in zip(w_,l_):\n",
        "    x_grid = np.linspace(-1.3,1.3,100)\n",
        "    y_pred = f_poly(x_grid,w,deg)\n",
        "    plt.plot(x_grid,y_pred,label=r'$\\lambda$ = %s'%l)\n",
        "plt.scatter(x,y,s=75,label='data',zorder=2.5)\n",
        "plt.legend()\n",
        "plt.xlabel(r'$x$',fontsize=18)\n",
        "plt.ylabel(r'$f(x)$',fontsize=18)\n",
        "# plt.savefig('Figures/polyfit_regularization.png',dpi=1800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_hSTGX2wJXh"
      },
      "source": [
        "### **Homework - Coefficient of determination $R^2$**\n",
        "\n",
        "The coefficient of determination, commonly denoted as $R^2$, is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. In simpler terms, it indicates how well data points fit a statistical model – in this case, a linear model.\n",
        "\n",
        "1. **Formula for $R^2$**:\n",
        "   - $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
        "   - This formula calculates $R^2$ as the proportion of the total variance in the dependent variable $(y)$ that is predictable from the independent variable $(x)$.\n",
        "\n",
        "2. **Residual Sum of Squares $SS_{res}$**:\n",
        "   - $SS_{res} = \\sum_{i} (y_i - f(x_i))^2$\n",
        "   - This is the sum of the squares of the residuals - the differences between the actual values $y_i$ and the values predicted by the model $f(x_i)$.\n",
        "   - It measures the total deviation of the response values from the fit to the observed data points.\n",
        "\n",
        "3. **Total Sum of Squares $SS_{tot}$**:\n",
        "   - $SS_{tot} = \\sum_{i} (y_i - \\hat{y})^2$\n",
        "   - Here, $\\hat{y}$ is the mean of the observed data: $\\hat{y} = \\frac{1}{N}\\sum_i^{N} y_i $\n",
        "   - $SS_{tot}$ measures the total variance in the response variable that needs to be explained by the model and by the residuals.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piGkCcrGyBjp"
      },
      "outputs": [],
      "source": [
        "# Code here\n",
        "def f_r_square(data,l,deg):\n",
        "\n",
        "    # solve for theta\n",
        "\n",
        "    # compute ss_res\n",
        "\n",
        "    # compute ss_tot\n",
        "\n",
        "    r2 = #\n",
        "    return r2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
