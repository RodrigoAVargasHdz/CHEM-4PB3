{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open on Google Colab [link](https://colab.research.google.com/github/RodrigoAVargasHdz/CHEM-4PB3/blob/main/Course_Notes/Week7/gradentdescent_torch.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model (recap)\n",
    "\n",
    "In the previous lectures, we studied linear model,\\\n",
    "$f_{\\phi(\\cdot)}(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) = \\sum_i w_i \\phi_i(\\mathbf{x})$.\n",
    "\n",
    "* $\\phi(\\cdot)$ is the linear map to a new **feature representation**, for example, polynomials, Fourier basis, ec.\n",
    "\n",
    "For a square loss function, $ {\\cal L} \\approx \\|\\mathbf{y} - f_{\\phi(\\cdot)}(\\mathbf{X}) \\|_2$, the optimal parameters $\\mathbf{w}^*$ can be obtained in closed form.\\\n",
    "\n",
    "$\\mathbf{w}^* = \\Big( \\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X})\\Big)^{-1} \\Phi(\\mathbf{X})^\\top\\mathbf{y}$.\n",
    "\n",
    "Theoretically, the bottleneck to solve for $\\mathbf{w}^*$ relies on inverting the matrix $\\Big( \\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X})\\Big)$.\\\n",
    "Matrix inversion is one of the most **expensive operations**, ${\\cal O}(N^3)$ where $N$ is this case is the number of training points.\n",
    "\n",
    "Let's assume we have a **GIGANTIC** dataset, $N \\approx 100,000$ points. \n",
    "It will computationally demanding to solve for $\\mathbf{w}^*$. \n",
    "\n",
    "However, the gradient of ${\\cal L}(\\mathbf{w})$ with respect to $\\mathbf{w}$ scales **quadratically**,\\\n",
    "$\\nabla {\\cal L}(\\mathbf{w}) = \\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X})\\mathbf{w} -\\Phi(\\mathbf{X})^\\top\\mathbf{y}$\\\n",
    "Matrix-matrix multiplication $(\\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X}))$ scales  ${\\cal O}(N^2)$, (**one order of magnitude lower**).\n",
    "\n",
    "\n",
    "* **$N = 100, \\;\\; N^2 = 10,000, \\;\\; N^3 = 1,000,000$**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "We can search for the minimum/maximum of $\\nabla {\\cal L}(\\mathbf{w})$ using gradient-based methods.\\\n",
    "For example, for $f(x_0,x_1) = x_0^2 + x_1^2$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sqr = lambda x:np.sum(np.power(x,2),axis=1)\n",
    "x = np.linspace(-2., 2., 50)\n",
    "x0,x1 = np.meshgrid(x,x)\n",
    "X = np.column_stack((x0.flatten(),x1.flatten()))\n",
    "y = f_sqr(X)\n",
    "y = y.reshape((x0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f'$f(x_0,x_1) = x_0^2 + x_1^2$')\n",
    "plt.contourf(x0, x1, y, levels=60)\n",
    "plt.xlabel(f'$x_0$', fontsize=15)\n",
    "plt.ylabel(f'$x_1$', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients \n",
    "The gradient (Jacobian) of $f(x_0,x_1) = x_0^2 + x_1^2$ is,\\\n",
    "$\\frac{\\partial f}{\\partial x_0} = 2x_0$ and $\\frac{\\partial f}{\\partial x_1} = 2x_1$.\\\n",
    "Meaning, $\\nabla f(x_0,x_1) = [2x_0,2x_1]$\n",
    "\n",
    "At any given point, $\\nabla f(x_0,x_1)$ tells you the direction in which the function changes with the greatest rate.\\\n",
    "If you think of the function as height, then it gives the direction in which the ground is steepest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f_sqr = lambda x:2*x\n",
    "\n",
    "gx = np.random.uniform(low=-0.75,high=0.75,size=(10,2))\n",
    "gx0,gx1 = gx[:,0],gx[:,1]\n",
    "gX = np.column_stack((gx0.flatten(), gx1.flatten()))\n",
    "print(gX.shape)\n",
    "grad = grad_f_sqr(gX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient field\n",
    "fig, ax = plt.subplots(1, 1,figsize=(8,8))\n",
    "ax.set_aspect(1)\n",
    "# ax.plot(feature_x, feature_y, c='k')\n",
    "gf_x0 = grad[:,0].reshape(gx0.shape)\n",
    "gf_x1 = grad[:,1].reshape(gx1.shape)\n",
    "ax.contourf(x0, x1, y, lw=2, levels=60)\n",
    "\n",
    "eta = 0.1 #rescale\n",
    "ax.quiver(gx0, gx1, eta*gf_x0, eta*gf_x1,\n",
    "          units='xy', scale=0.15, color='white')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(r'$-\\nabla f(x_0,x_1)$')\n",
    "ax.set_xlabel(r'$\\frac{\\partial f}{\\partial x_{0}}$', fontsize=15)\n",
    "ax.set_ylabel(r'$\\frac{\\partial f}{\\partial x_{1}}$', fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum search using gradient descent\n",
    "Algorithm\n",
    "1. Start with a random initial value of $x_0$ and $x_1$; $[x_0,x_1]_{t}$\n",
    "2. Compute the gradient; $\\nabla f(x_0,x_1) = \\nabla f([x_0,x_1]_{t})$\n",
    "3. Update $x_0$ and $x_1$ using the information from step 2.\n",
    "    * $[x_0,x_1]_{t+1} = [x_0,x_1]_t - \\eta \\nabla f(x_0,x_1)$ \n",
    "  * $\\eta$ is known as the step size or **learning rate**\n",
    "1. Repeat until convergence criteria is is satisfied:\n",
    "    * $\\|\\nabla f(x_0,x_1) \\| < 10^{-6}$\n",
    "    *  $\\| f([x_0,x_1]_{t+1}) - f([x_0,x_1]_{t})\\| < 10^{-6} $\n",
    " * $10^{-6}$ is a threshold  parameter, and is problem dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent algorithm\n",
    "x = np.random.uniform(-2.,2.,(1,2))\n",
    "eta = 0.1\n",
    "\n",
    "x_trajectory = np.zeros_like(x.ravel())\n",
    "g_trajectory = np.zeros_like(x.ravel())\n",
    "f_trajectory = np.zeros(1)\n",
    "c = 1E-4\n",
    "xt = x\n",
    "for t in range(20):\n",
    "    grad_t = grad_f_sqr(xt)\n",
    "    y_t = f_sqr(xt)\n",
    "    g_trajectory = np.vstack((g_trajectory,grad_t))\n",
    "    f_trajectory = np.append(f_trajectory, y_t)\n",
    "    x_trajectory = np.vstack((x_trajectory, xt))\n",
    "    print('t = %s, f = %.4f, '%(t,y_t),'x = ', x   ,'g = ', grad_t)\n",
    "    xt = xt - eta*grad_t\n",
    "    \n",
    "    if np.linalg.norm(g_trajectory[-2] - g_trajectory[-1]) < c or np.linalg.norm(f_trajectory[-2] - f_trajectory[-1]) < c:\n",
    "        break\n",
    "\n",
    "# remove initial useless point.\n",
    "g_trajectory = g_trajectory[1:]\n",
    "f_trajectory = f_trajectory[1:]\n",
    "x_trajectory = x_trajectory[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1,figsize=(8,8))\n",
    "ax.set_aspect(1)\n",
    "# ax.plot(feature_x, feature_y, c='k')\n",
    "gf_x0 = grad[:,0].reshape(gx0.shape)\n",
    "gf_x1 = grad[:,1].reshape(gx1.shape)\n",
    "ax.contourf(x0, x1, y, lw=2, levels=60)\n",
    "ax.plot(x_trajectory[:,0],x_trajectory[:,1],color='red',marker='o',label=r'$[x_0,x_1]_t$')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel(r'$x_0$',fontsize=15)\n",
    "ax.set_ylabel(r'$x_1$',fontsize=15)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happen when we do not have gradients $\\nabla f(\\mathbf{x})$?\n",
    "\n",
    "(extra topic), study [Finite Difference](https://mathworld.wolfram.com/FiniteDifference.html)\n",
    "\n",
    "Finite Difference --> numerical approximation for gradients and higher order derivatives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's revise **linear model**,\n",
    "$f_{\\phi(\\cdot)}(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) = \\sum_i w_i \\phi_i(\\mathbf{x})$.\n",
    "\n",
    "**Diagram**\\\n",
    "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/Linear_model_diagram.png\"  width=\"350\" height=\"300\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model in Torch for Harmonic series\n",
    "\n",
    "Harmonic series to approximate the Morse function. [**Reference**](https://www.tandfonline.com/doi/full/10.1080/08927020903433739)\\\n",
    "**Morse function**\\\n",
    "$E_{Morse} = D_{e}\\left (1-\\exp^{-\\alpha(r-r_0)} \\right )^2$,\\\n",
    "where $\\alpha = \\sqrt{k/2D_{e}}$.\n",
    "\n",
    "**Harmonic series**\\\n",
    "$E_{harmonic} = \\sum_{\\ell=1}^p \\frac{k_\\ell}{r^{\\ell}}$, \n",
    "is alternative to the polynomials that approaches to zero as $r$ approaches to infinity.\n",
    "\n",
    "Computationally, $E_{harmonic}$ is simply a linear model over $[r,r^1,\\cdots,r^\\ell]$, and the parameters $k_\\ell$ are linear weigths.\\\n",
    "(let's code this in torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def polynomial(r,ell):\n",
    "    r = 1/r #power ell = 1\n",
    "    R = r\n",
    "    # loop over higher orders\n",
    "    for i in range(2,ell+1):\n",
    "        r_l = np.power(r,i)\n",
    "        R = np.column_stack((R,r_l))\n",
    "    return R\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in torch\n",
    "class HarmonicSeries(nn.Module):\n",
    "    def __init__(self, ell):\n",
    "        super().__init__()\n",
    "        self.ell = ell\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.div(1,x)\n",
    "        R = x\n",
    "        for i in range(2,self.ell+1):\n",
    "            r_l = x.pow(i)\n",
    "            R = torch.column_stack((R,r_l))\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHarmonicModel(nn.Module):\n",
    "    def __init__(self,ell):\n",
    "        super().__init__()\n",
    "        self.ell = ell\n",
    "        self.harmonic = HarmonicSeries(self.ell)\n",
    "        self.linear = nn.Linear(self.ell,1)\n",
    "        nn.init.zeros_(self.linear.weight) #play around with this initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.harmonic(x)\n",
    "        y = self.linear(r)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit N2 PES (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "data_url = \"https://github.com/RodrigoAVargasHdz/CHEM-4PB3/raw/main/Course_Notes/data/PES_N2\"\n",
    "data = pd.read_csv(data_url)\n",
    "X, y = data['R'].to_numpy(), data['Energy'].to_numpy()\n",
    "X = torch.from_numpy(X).unsqueeze(1).float()\n",
    "y = (y - np.mean(y))/np.std(y)\n",
    "y = torch.from_numpy(y).unsqueeze(1).float()\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our model\n",
    "model = LinearHarmonicModel(20)\n",
    "X_rnd = torch.randn((10, 1))\n",
    "y_pred = model(X)\n",
    "loss = nn.MSELoss()\n",
    "print(loss(y_pred,y))\n",
    "print(y_pred.shape)\n",
    "print('Parameters')\n",
    "print(model.linear.weight)\n",
    "print(model.linear.bias)\n",
    "\n",
    "\n",
    "plt.plot(X.numpy(),y.numpy())\n",
    "plt.plot(X.numpy(),model(X).detach().numpy())\n",
    "plt.xlabel('R(N-N)')\n",
    "plt.ylabel('Energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a linear model\n",
    "m = nn.Linear(10,2) # input dimensions -> output dimensions\n",
    "print(m.weight)\n",
    "print(m.bias)\n",
    "X = torch.randn((5,10))\n",
    "y = X @ m.weight.T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# Use the adam optimizer\n",
    "# Includes GaussianLikelihood parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "training_iter = 2000\n",
    "loss_trajectory = []\n",
    "loss_trajectory_tst = []\n",
    "\n",
    "l2_lambda = 0.1\n",
    "for itr in range(1,training_iter):\n",
    "    output = model(X)\n",
    "    loss_val = loss(output,y)\n",
    "    \n",
    "    # l_norm = sum(p.pow(2.0).sum()\n",
    "    #               for p in model.parameters())\n",
    "    l_norm = sum(p.abs().sum()for p in model.parameters())\n",
    "    \n",
    "    loss_val = loss_val + l2_lambda*l_norm\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if itr%10 == 0 :\n",
    "        print(f'itr = %s, loss = %.4f'%(itr,loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(X.numpy()[::2], y.numpy()[::2], marker='s', markersize=5,label='DFT')\n",
    "\n",
    "X_grid = torch.linspace(X.min(),X.max(),2000)\n",
    "X_grid = X_grid.unsqueeze(-1)\n",
    "plt.plot(X_grid.detach().numpy(), model(X_grid).detach().numpy(),label='Harmonic Series')\n",
    "plt.xlabel('R(N-N)',fontsize=15)\n",
    "plt.ylabel('Energy',fontsize=15)\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next lecture\n",
    "\n",
    "## What if $\\phi(\\cdot)$ also depends on internal paramters? **non-linear model**,\n",
    "$f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x},\\mathbf{w}') = \\sum_i w_i \\phi_i(\\mathbf{x},\\mathbf{w}')$.\n",
    "\n",
    "Now we also need to optimize the non-linear parameters $\\mathbf{w}'$.\n",
    "\n",
    "**Diagram**\\\n",
    "<img src=\"https://raw.github.com/RodrigoAVargasHdz/CHEM-4PB3/master/Course_Notes/Figures/nonLinear_model_diagram.png\"  width=\"350\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem4pb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bd1d19d093f1d1d5dd6cf1becc7a1cffecc3235fce6b74420ac04427a66c9c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
