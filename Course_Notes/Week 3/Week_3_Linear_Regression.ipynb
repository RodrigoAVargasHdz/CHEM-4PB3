{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/RodrigoAVargasHdz/CHEM-4PB3/blob/w2024/Course_Notes/Week%203/Week_3_Linear_Regression.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f(x) = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d = \\begin{bmatrix}\n",
    "    w_0 & w_1 & \\cdots & w_p \\\\\n",
    "    \\end{bmatrix} \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    x_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_d\n",
    "      \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **d** ->  is the number of dimensions or features in $\\mathbf{x}$\n",
    "* $\\mathbf{w}$ -> parameters of the linear model\n",
    "\n",
    "# Loss function #\n",
    "* Quantify the accuracy of the linear model.\n",
    "  $$\n",
    "    \\cal{L}(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( f(\\mathbf{x}_i) - y_i \\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\left( \\mathbf{w}^\\top \\mathbf{x}_i - y_i \\right)^2 \n",
    "  $$\n",
    "  $$\n",
    "    \\cal{L}(\\mathbf{w}) = \\frac{1}{2} \\left (\\mathbf{y} - \\mathbf{X} \\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)\n",
    "  $$\n",
    "What is $\\mathbf{X}$ and $\\mathbf{y}$?\n",
    "$$\n",
    "   \\mathbf{X} = \\begin{pmatrix}\n",
    "[ x_1^0, & x_1^1, & \\cdots,& x_1^{d-1}, & x_1^d ] \\\\ \n",
    "[ x_2^0, & x_2^1, & \\cdots,& x_2^{d-1}, & x_2^d]  \\\\ \n",
    "  & & \\cdots & &  \\\\\n",
    "[ x_{N-1}^0, & x_{N-1}^1, & \\cdots,& x_{N-1}^{d-1}, & x_{N-1}^d ] \\\\\n",
    "[ x_{N}^0, & x_{N-1}^1, & \\cdots,& x_{N}^{d-1}, & x_{Nt}^d ] \n",
    "\\end{pmatrix} =  \\begin{pmatrix}\n",
    "\\mathbf{x}_1^\\top\\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_n^\\top\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "   \\mathbf{y} =  \\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots \\\\\n",
    "y_n\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data over f(x) = sin(x) + x - 1\n",
    "def get_data(N,bool_biased=True):\n",
    "    x = np.linspace(-1.,1.,N) #This creates an array x of N linearly spaced values between -1 and 1.\n",
    "    y = np.sin(.5*x) + x -1.\n",
    "    y = y + np.random.uniform(low = 0.,high=0.5,size=x.shape) #Adds random noise to each y value.\n",
    "    if bool_biased:\n",
    "        X = np.column_stack((np.ones_like(x),x))\n",
    "    else:\n",
    "        X = x[:,None]\n",
    "    return X,y\n",
    "\n",
    "X,y = get_data(10) \n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look closer to the term $\\mathbf{X}\\mathbf{w}$,\n",
    "$$\n",
    " \\mathbf{X}\\mathbf{w}  =  \\begin{pmatrix}\n",
    "\\mathbf{x}_1^T\\\\ \n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_N^T\n",
    "\\end{pmatrix} \\mathbf{w} = \\begin{pmatrix}\n",
    "[ x_1^0, & \\cdots,& x_1^d ] \\\\ \n",
    "\\cdots  & \\cdots & \\cdots \\\\\n",
    "[ x_N^0, & \\cdots, & x_N^d ] \n",
    "\\end{pmatrix} \\begin{pmatrix} \n",
    "w_0\\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random parameters\n",
    "def get_random_params(d):\n",
    "    theta_random = np.random.uniform(low=-2., high=2., size=(d))\n",
    "    return theta_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = get_random_params(2)\n",
    "print('Parameters: ', w)\n",
    "\n",
    "# what operation is Xw?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Solution of Linear Regression ##\n",
    "\n",
    "* **Gradient of a function equal to zero means a maxima or minima**\n",
    "  \n",
    "$$\n",
    "    \\nabla {\\cal L}(\\mathbf{w}) \\Big\\rvert_{\\mathbf{w}^{*}} = \\frac{1}{2} \\nabla_{\\mathbf{w}} \\left [ \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right) \\right ]= 0\n",
    "$$\n",
    "\n",
    "To solve for $\\mathbf{w}^*$, let's expand $ \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)$,\n",
    "\n",
    "$$\n",
    "    \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right) ={\\color{red} \\mathbf{y}^\\top \\mathbf{y}}  - {\\color{blue}\\mathbf{y}^\\top \\mathbf{X}\\mathbf{w}} -  {\\color{blue}\\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{y}} +   {\\color{blue}\\mathbf{w}^\\top\\mathbf{X}^\\top \\mathbf{X}\\mathbf{w}}\n",
    "$$\n",
    "$$\n",
    "    \\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w}) = \\frac{1}{2}\\left(  -2 \\mathbf{X}^\\top\\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} \\right) = 0\n",
    "$$\n",
    "\n",
    "**Extra:**\n",
    "1. Homework, Proof the above equations.\n",
    "2. [Equations from Sections 2.4.1 and 2.4.2](https://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving for $\\mathbf{w}$,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w}) &= \\frac{1}{2}\\left(  -2 \\mathbf{X}^\\top\\mathbf{y} + 2{\\color{blue}\\mathbf{X}^\\top\\mathbf{X}} \\mathbf{w} \\right) = 0 \\\\\n",
    " {\\color{blue}\\mathbf{X}^\\top\\mathbf{X}} \\mathbf{w}  &=  \\mathbf{X}^\\top\\mathbf{y} \\\\\n",
    "  \\mathbf{w}^* &= \\left ( {\\color{blue}\\mathbf{X}^\\top\\mathbf{X}}\\right ) ^{-1} \\mathbf{X}^\\top\\mathbf{y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**What is $\\mathbf{X}^\\top\\mathbf{X}$ ?**\n",
    "\n",
    "$$\n",
    " \\mathbf{X}^\\top\\mathbf{X} = \\begin{pmatrix}\n",
    " x_1^0 &  x_2^0 & \\cdots& x_N^0  \\\\ \n",
    "  x_1^1 &  x_2^1 & \\cdots& x_N^1  \\\\ \n",
    "\\vdots &  \\vdots & \\vdots& \\vdots  \\\\ \n",
    "  x_{1}^{d-1} &  x_{2}^{d-1}& \\cdots& x_{N}^{d-1}  \\\\ \n",
    "    x_{1}^{d} &  x_{2}^{d}& \\cdots& x_{N}^{d} \n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "[ x_1^0, & x_1^1, & \\cdots,& x_1^{d-1}, & x_1^d ] \\\\ \n",
    "[ x_2^0, & x_2^1, & \\cdots,& x_2^{d-1}, & x_2^d]  \\\\ \n",
    "  & & \\cdots & &  \\\\\n",
    "[ x_{N-1}^0, & x_{N-1}^1, & \\cdots,& x_{N-1}^{d-1}, & x_{N-1}^d ] \\\\\n",
    "[ x_{N}^0, & x_{N-1}^1, & \\cdots,& x_{N}^{d-1}, & x_{Nt}^d ] \n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\mathbf{x}_1 & \\cdots &  \\mathbf{x}_N \\end{pmatrix}  \\begin{pmatrix}\n",
    "\\mathbf{x}_1^\\top\\\\ \n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_N^\\top\n",
    "\\end{pmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code X^t X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters ##\n",
    "\n",
    "$$\n",
    " \\mathbf{w}^* = \\left ( \\mathbf{X}^\\top \\mathbf{X}^\\top \\right ) ^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "### Operations ###\n",
    "1. $\\mathbf{X}^\\top$ -> matrix transpose \n",
    "2. $\\mathbf{X}^\\top\\mathbf{y}$ -> matrix-vector multiplication \n",
    "3. $\\left (\\mathbf{X}^\\top\\mathbf{X}\\right ) ^{-1}$ -> matrix inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inclass excercise\n",
    "def linear_model_solver(X,y):\n",
    "    \n",
    "    return w # optmal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our model\n",
    "X, y = get_data(25)\n",
    "w = linear_model_solver(X,y)\n",
    "\n",
    "x_grid = np.linspace(-1.,1.,250)\n",
    "X_grid = np.column_stack((np.ones_like(x_grid), x_grid))\n",
    "y_pred = #complete\n",
    "\n",
    "plt.scatter(X[:,-1], y, label='data')\n",
    "plt.plot(x_grid, y_pred,label='prediction')\n",
    "plt.xlabel(r'$x$', fontsize=18)\n",
    "plt.ylabel(r'$f(x)$', fontsize=18)\n",
    "plt.ylim(-3., 2.)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Linear Models ##\n",
    "\n",
    "Let's revise polynomials:\n",
    "\n",
    "How many terms if we have a second-order polynomial and $d=3$?\n",
    "$$\n",
    "\\begin{align}\n",
    "(1+x_1+x_2+x_3)^3 &= (1+x_1+x_2+x_3)(1+x_1+x_2+x_3)^2 \\\\\n",
    "&= 1+3x_1+3x^2_1+x^3_1+3x_2+6x_1x_2+3x^2_1x_2 \\\\ \n",
    "& +3x_2^2+3x_1x_2^2+x_2^3 +3x_3+6x_1x_3+3x_1^2x_3 \\\\\n",
    "& +6x_2x_3+6x_1x_2x_3+3x_2^2x_3+3x_3^2 \\\\\n",
    "& +3x_1x_3^2+3x_2x_3^2+x_3^3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is simply a new representation of $x$\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = [1, x_1, x_2, x_3, \\cdots, x_i x_j, \\cdots, x_i^{ m} x_j^{p}, \\cdots, x_i^{ m} x_j^{p}x_{\\ell}^{r}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear models on basis-set expansion**\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x},\\mathbf{w}) = \\sum_{i=0}^d w_i \\phi(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "* **Loss function**,\n",
    "$$\n",
    "    \\begin{align}\n",
    "    {\\cal L}(\\mathbf{w}) &= \\frac{1}{2}\\sum_i^N (y_i - f(\\mathbf{x}_i,\\mathbf{w}))^2 = \\frac{1}{2}\\sum_i^N (y_i - \\mathbf{w}^\\top \\phi(\\mathbf{x}_i))^2 \\\\\n",
    "    &= \\frac{1}{2} \\left (\\mathbf{y} - \\Phi(\\mathbf{x})\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} -  \\Phi(\\mathbf{x})\\mathbf{w} \\right)\n",
    "    \\end{align}\n",
    "$$\n",
    "Homework, proof the above equations.\n",
    "\n",
    "\n",
    "1. What is $\\Phi(\\mathbf{x})$?\n",
    "2. What is the form of the **optimal** parameters $\\mathbf{w}^*$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X,deg):\n",
    "    poly = PolynomialFeatures(deg) \n",
    "    Phi = poly.fit_transform(X)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_model_solver(X,y,deg):\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with last lecture's example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "X, y = get_data(N)\n",
    "x_grid = np.linspace(-2.,2.,250)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "w_ = [] # to save the number of parameters\n",
    "p_ = np.arange(1, 13, 1, dtype=np.int32) # polynomial degrees\n",
    "for p in p_:  # loop over different degrees\n",
    "    w = polynomial_model_solver(X,y,p)\n",
    "    Phi = polynomial_features(X,p)\n",
    "    y_pred = #complete prediction use x_grid\n",
    "    \n",
    "    plt.plot(x_grid, y_pred, label='p=%s' % p)\n",
    "    w_.append(np.pad(w, (0, 13-w.shape[0]),\n",
    "              mode='constant', constant_values=0))\n",
    "plt.scatter(X[:,-1], y, s=75, label='data')\n",
    "plt.legend(fontsize=5)\n",
    "plt.xlabel(r'$x$', fontsize=18)\n",
    "plt.ylabel(r'$f(x)$', fontsize=18)\n",
    "# plt.savefig('Figures/polyfit_2.png',dpi=1800)\n",
    "\n",
    "fig, ax0 = plt.subplots(1, 1)\n",
    "c = ax0.pcolor(np.abs(np.array(w_)), edgecolors='k', linewidths=4)\n",
    "fig.colorbar(c, ax=ax0, label=r'$|w_i|$')\n",
    "ax0.set_xlabel(r'$w_i$')\n",
    "ax0.set_yticks(np.arange(p_.shape[0])+0.5, p_)\n",
    "ax0.set_ylabel('Poly degree')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra ##\n",
    "\n",
    "Let's revise the operation $\\mathbf{X}\\mathbf{w}$. \n",
    "\n",
    "$$\n",
    " \\mathbf{X}\\mathbf{w}  =  \\begin{pmatrix}\n",
    "\\mathbf{x}_1^T\\\\ \n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_N^T\n",
    "\\end{pmatrix} \\mathbf{w} = \\begin{pmatrix}\n",
    "[ x_1^0, & \\cdots,& x_1^d ] \\\\ \n",
    "\\cdots  & \\cdots & \\cdots \\\\\n",
    "[ x_N^0, & \\cdots, & x_N^d ] \n",
    "\\end{pmatrix} \\begin{pmatrix} \n",
    "w_0\\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem4pb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
